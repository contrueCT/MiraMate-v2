from math import fabs
import os
import json
from typing import Dict, Any

from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.language_models.chat_models import BaseChatModel
from sympy import false
from MiraMate.modules.settings import get_project_root as _settings_project_root

PROJECT_ROOT = str(_settings_project_root())
LLM_CONFIG_PATH = os.path.join(PROJECT_ROOT, "configs", "llm_config.json")

def create_llm_from_config(config: Dict[str, Any], is_main_llm: bool = False, **kwargs) -> BaseChatModel:
    """
    æ ¹æ®å•æ¡é…ç½®å­—å…¸ï¼ŒåŠ¨æ€åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªLangChain LLMå®ä¾‹ã€‚

    :param config: åŒ…å« 'api_type', 'model', 'api_key' ç­‰çš„å­—å…¸ã€‚
    :param kwargs: é¢å¤–çš„ã€è¦ä¼ é€’ç»™LLMæ„é€ å‡½æ•°çš„å‚æ•° (ä¾‹å¦‚ streaming=True)ã€‚
    :return: ä¸€ä¸ª BaseChatModel çš„å®ä¾‹ã€‚
    """
    api_type = config.get("api_type")
    model_name = config.get("model")
    api_key = config.get("api_key")  # API Key ä»…æ¥æºäºé…ç½®æ–‡ä»¶ï¼Œç§»é™¤ç¯å¢ƒå˜é‡å…œåº•
    base_url = config.get("base_url")

    if not all([api_type, model_name, api_key]):
        raise ValueError(f"é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘ 'api_type', 'model', æˆ– 'api_key': {config}")

    # åˆå¹¶é»˜è®¤kwargså’Œä¼ å…¥çš„kwargsï¼Œä¼ å…¥çš„ä¼˜å…ˆ
    final_kwargs = {**config.get("model_kwargs", {}), **kwargs}

    print(f"æ­£åœ¨åˆ›å»ºæ¨¡å‹å®ä¾‹: [Type: {api_type}, Model: {model_name}]")

    # --- æ ¹æ® api_type åŠ¨æ€é€‰æ‹©è¦å®ä¾‹åŒ–çš„ç±» ---
    if api_type == "openai":
        return ChatOpenAI(
            model=model_name,
            api_key=api_key,
            base_url=base_url,
            # åªæœ‰ä¸»æ¨¡å‹æ‰é»˜è®¤å¼€å¯æµå¼
            streaming=is_main_llm, 
            **final_kwargs
        )
    elif api_type == "gemini":
        # å¯¹äº Geminiï¼Œæ„é€ å‡½æ•°ä¸­æ²¡æœ‰ streaming å‚æ•°ã€‚
        # LangChain ä¼šåœ¨è°ƒç”¨ .stream() æ—¶è‡ªåŠ¨å¤„ç†æµå¼è¯·æ±‚ã€‚
        return ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=api_key,
            **final_kwargs
        )
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ API ç±»å‹: '{api_type}'ã€‚ç›®å‰ä»…æ”¯æŒ 'openai' å’Œ 'gemini'ã€‚")

def load_llms_from_json(config_path: str) -> tuple[BaseChatModel, BaseChatModel]:
    """
    ä»JSONé…ç½®æ–‡ä»¶åŠ è½½ä¸»æ¨¡å‹å’Œæ¬¡æ¨¡å‹ã€‚
    çº¦å®šï¼šåˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªé…ç½®ä¸º main_llmï¼Œç¬¬äºŒä¸ªä¸º small_llmã€‚
    """
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            configs = json.load(f)
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° LLM é…ç½®æ–‡ä»¶ '{config_path}'ã€‚")
        print("ğŸ’¡ å·²ä¸ºæ‚¨åˆ›å»ºä¸€ä¸ªæ¨¡æ¿æ–‡ä»¶ï¼Œè¯·å¡«å†™æ‚¨çš„ API ä¿¡æ¯åå†é‡æ–°è¿è¡Œã€‚")
        # åˆ›å»ºä¸€ä¸ªæ¨¡æ¿æ–‡ä»¶å¼•å¯¼ç”¨æˆ·
        template_config = [
            {
                "model": "gpt-4o",
                "api_key": "YOUR_OPENAI_API_KEY",
                "base_url": "https://api.openai.com/v1",
                "api_type": "openai",
                "model_kwargs": {"temperature": 0.8}
            },
            {
                "model": "gemini-1.5-flash-latest",
                "api_key": "YOUR_GEMINI_API_KEY",
                "api_type": "gemini",
                "model_kwargs": {"temperature": 0.0}
            }
        ]
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(template_config, f, ensure_ascii=False, indent=4)
        return None, None
    except json.JSONDecodeError:
        print(f"âŒ é”™è¯¯: LLM é…ç½®æ–‡ä»¶ '{config_path}' æ ¼å¼é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„JSONã€‚")
        return None, None

    if not isinstance(configs, list) or not configs:
        print(f"âŒ é”™è¯¯: '{config_path}' çš„å†…å®¹å¿…é¡»æ˜¯ä¸€ä¸ªéç©ºçš„JSONåˆ—è¡¨ã€‚")
        return None, None

    # --- æ ¹æ®çº¦å®šåˆ›å»º main å’Œ small æ¨¡å‹ ---
    # ç¬¬ä¸€ä¸ªä½œä¸ºä¸»æ¨¡å‹
    main_config = configs[0]
    # ç¬¬äºŒä¸ªä½œä¸ºæ¬¡æ¨¡å‹ï¼›å¦‚æœåªæœ‰ä¸€ä¸ªï¼Œåˆ™æ¬¡æ¨¡å‹ä¹Ÿä½¿ç”¨ä¸»æ¨¡å‹çš„é…ç½®
    small_config = configs[1] if len(configs) > 1 else configs[0]

    # ä¸ºä¸»æ¨¡å‹å’Œæ¬¡æ¨¡å‹è®¾ç½®ä¸åŒçš„é»˜è®¤å‚æ•°
    # ä¸»æ¨¡å‹ç”¨äºç”Ÿæˆï¼Œéœ€è¦æµå¼å’Œåˆ›é€ æ€§
    main_llm_instance = create_llm_from_config(main_config,is_main_llm=True)
    
    # æ¬¡æ¨¡å‹ç”¨äºåˆ†æå’ŒJSONè¾“å‡ºï¼Œéœ€è¦ç¡®å®šæ€§å’Œç‰¹å®šæ ¼å¼
    small_llm_instance = create_llm_from_config(
        small_config,
        # å¦‚æœæ˜¯ openai å…¼å®¹çš„ apiï¼Œå¯ä»¥å¼ºåˆ¶ json è¾“å‡º
        model_kwargs={"response_format": {"type": "json_object"}} if small_config.get("api_type") == "openai" else {},
        is_main_llm=False
    )

    return main_llm_instance, small_llm_instance

# --- æ¨¡å—ä¸»é€»è¾‘ï¼šåŠ è½½æ¨¡å‹å¹¶å¯¼å‡º ---
main_llm, small_llm = load_llms_from_json(LLM_CONFIG_PATH)

if main_llm is None or small_llm is None:
    print("âš ï¸ ç”±äºæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œåº”ç”¨å¯èƒ½æ— æ³•æ­£å¸¸è¿è¡Œã€‚è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶ã€‚")
else:
    print("âœ… ä¸»æ¨¡å‹å’Œæ¬¡æ¨¡å‹å·²æˆåŠŸåŠ è½½ï¼")