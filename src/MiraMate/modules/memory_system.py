# å·²ç»æ¥å…¥äº†å¯¹è¯é“¾ï¼Œä¸»è¦è´Ÿè´£ç”¨æˆ·ç”»åƒã€å¯¹è¯è®°å¿†ã€äº‹å®è®°å¿†ç­‰çš„å­˜å‚¨å’Œæ£€ç´¢ï¼Œåç»­éœ€è¦åœ¨å¯¹è¯å®Œåçš„æ­¥éª¤ä¸­æ›´æ–°è®°å¿†å†…å®¹ï¼šå¯¹è¯è®°å¿†ã€äº‹å®è®°å¿†ã€ç”¨æˆ·åå¥½ã€é‡å¤§äº‹ä»¶ç­‰ã€‚

import json
import os
# åœ¨å¯¼å…¥ä»»ä½•æ¨¡å‹å‰è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_DATASETS_OFFLINE"] = "1"

import chromadb
from datetime import datetime, timezone
from typing import List, Dict, Optional
from uuid import uuid4
from chromadb.utils import embedding_functions

# === ğŸ§  ä¸€ã€é€šç”¨ç»“æ„å®šä¹‰ ===

def get_timestamp():
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")  # å¯è¯»æ—¶é—´æˆ³

def get_iso_timestamp():
    return datetime.now().isoformat()  # ISOæ ¼å¼æ—¶é—´æˆ³ï¼Œç”¨äºChromaDB

def format_natural_time(dt: datetime) -> str:
    """å°†æ—¶é—´æ ¼å¼åŒ–ä¸ºè‡ªç„¶è¯­è¨€å½¢å¼"""
    weekdays = ['æ˜ŸæœŸä¸€', 'æ˜ŸæœŸäºŒ', 'æ˜ŸæœŸä¸‰', 'æ˜ŸæœŸå››', 'æ˜ŸæœŸäº”', 'æ˜ŸæœŸå…­', 'æ˜ŸæœŸæ—¥']
    weekday = weekdays[dt.weekday()]
    
    # åˆ¤æ–­æ—¶é—´æ®µ
    hour = dt.hour
    if 5 <= hour < 12:
        time_period = "ä¸Šåˆ"
    elif 12 <= hour < 14:
        time_period = "ä¸­åˆ"
    elif 14 <= hour < 18:
        time_period = "ä¸‹åˆ"
    elif 18 <= hour < 22:
        time_period = "æ™šä¸Š"
    else:
        time_period = "æ·±å¤œ"
    
    return f"{dt.year}å¹´{dt.month}æœˆ{dt.day}æ—¥{weekday}{time_period}{dt.hour}ç‚¹{dt.minute}åˆ†"

# === ğŸ“¦ äºŒã€å­˜å‚¨ç›®å½•è®¾ç½® - Dockerç¯å¢ƒé€‚é… ===
def get_project_root():
    """è·å–é¡¹ç›®æ ¹ç›®å½•ï¼Œæ”¯æŒDockerç¯å¢ƒ"""
    if os.getenv('DOCKER_ENV'):
        return '/app'
    # å¼€å‘ç¯å¢ƒï¼šä» modules/ å‘ä¸Š3çº§åˆ°é¡¹ç›®æ ¹ç›®å½•
    # å½“å‰æ–‡ä»¶: src/MiraMate/modules/memory_system.py
    # é¡¹ç›®æ ¹ç›®å½•: å‘ä¸Š3çº§
    MODULE_DIR = os.path.dirname(os.path.abspath(__file__))
    return os.path.abspath(os.path.join(MODULE_DIR, '..', '..', '..'))

PROJECT_ROOT = get_project_root()

# Dockerç¯å¢ƒé€‚é…çš„å­˜å‚¨è·¯å¾„
if os.getenv('DOCKER_ENV'):
    # Dockerç¯å¢ƒï¼šä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„å†…å­˜æ•°æ®åº“ç›®å½•
    MEMORY_DB_DIR = os.getenv('MEMORY_DB_DIR', '/app/memory_db')
    BASE_DIR = os.path.join(MEMORY_DB_DIR, "memory_storage")
else:
    # å¼€å‘ç¯å¢ƒï¼šä½¿ç”¨ç›¸å¯¹è·¯å¾„
    BASE_DIR = os.path.join(PROJECT_ROOT, "memory", "memory_storage")

PROFILE_PATH = os.path.join(BASE_DIR, "user_profile.json")
ACTIVE_TAGS_PATH = os.path.join(BASE_DIR, "active_tags.json")
TEMP_FOCUS_EVENTS_PATH = os.path.join(BASE_DIR, "temp_focus_events.json")

# ç¼“å­˜æ–‡ä»¶è·¯å¾„
PREFERENCE_CACHE_PATH = os.path.join(BASE_DIR, "preference_cache.json")
FACT_CACHE_PATH = os.path.join(BASE_DIR, "fact_cache.json")
PROFILE_CACHE_PATH = os.path.join(BASE_DIR, "profile_cache.json")

# ChromaDB å­˜å‚¨ç›®å½•
CHROMA_DB_DIR = os.path.join(BASE_DIR, "chroma_db")

# åˆ›å»ºå¿…è¦çš„ç›®å½•
os.makedirs(BASE_DIR, exist_ok=True)
os.makedirs(CHROMA_DB_DIR, exist_ok=True)

# === ğŸ”§ ä¸‰ã€ChromaDB åˆå§‹åŒ– ===

class MemorySystem:
    def __init__(self, persist_directory=None):
        """åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ"""
        if persist_directory is None:
            persist_directory = CHROMA_DB_DIR
        
        # åˆ›å»ºæŒä¹…åŒ–ç›®å½•
        os.makedirs(persist_directory, exist_ok=True)
        
        # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯
        self.client = chromadb.PersistentClient(path=persist_directory)
        print(f"âœ… ChromaDBå®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼ŒæŒä¹…åŒ–ç›®å½•: {persist_directory}")

        # é…ç½®åµŒå…¥å‡½æ•°
        self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="BAAI/bge-base-zh-v1.5",
            device="cpu"
        )

        # å®šä¹‰HNSWç´¢å¼•å‚æ•°
        self.hnsw_metadata_config = {
            "hnsw:space": "cosine",          # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            "hnsw:M": 32,                    # æ¨èMå€¼
            "hnsw:construction_ef": 256,     # construction_efå€¼
            "hnsw:num_threads": 4            # æ„å»ºçº¿ç¨‹æ•°
        }

        # åˆå§‹åŒ–é›†åˆ
        self.collections = {
            "dialog_logs": self.client.get_or_create_collection(
                name="dialog_logs",
                embedding_function=self.embedding_function,
                metadata=self.hnsw_metadata_config
            ),
            "facts": self.client.get_or_create_collection(
                name="facts",
                embedding_function=self.embedding_function,
                metadata=self.hnsw_metadata_config
            ),
            "user_preferences": self.client.get_or_create_collection(
                name="user_preferences",
                embedding_function=self.embedding_function,
                metadata=self.hnsw_metadata_config
            ),
            "important_events": self.client.get_or_create_collection(
                name="important_events",
                embedding_function=self.embedding_function,
                metadata=self.hnsw_metadata_config
            )
        }

    # --- å†…éƒ¨ï¼šå®‰å…¨æŸ¥è¯¢ + ç´¢å¼•è‡ªä¿®å¤ ---
    def _safe_query(self, collection_key: str, search_params: Dict):
        """
        å¯¹ Chroma é›†åˆæ‰§è¡ŒæŸ¥è¯¢ï¼Œè‹¥æ£€æµ‹åˆ° HNSW æ®µç´¢å¼•ä¸¢å¤±/æŸåï¼Œåˆ™è‡ªåŠ¨é‡å»ºå¹¶é‡è¯•ä¸€æ¬¡ã€‚
        ä¸æ”¹å˜åŸæœ‰æŸ¥è¯¢è¡Œä¸ºï¼Œåªæœ‰åœ¨ç‰¹å®šé”™è¯¯å‡ºç°æ—¶æ‰ä»‹å…¥è‡ªä¿®å¤ã€‚
        """
        try:
            return self.collections[collection_key].query(**search_params)
        except Exception as e:
            msg = str(e).lower()
            # å¸¸è§æŠ¥é”™ï¼š"error creating hnsw segment reader: Nothing found on disk"
            if ("hnsw" in msg) and ("segment" in msg or "nothing found on disk" in msg):
                print(f"\u26a0\ufe0f æ£€æµ‹åˆ°é›†åˆ '{collection_key}' çš„ HNSW ç´¢å¼•ç¼ºå¤±/æŸåï¼Œæ­£åœ¨å°è¯•è‡ªåŠ¨é‡å»º...")
                self._rebuild_collection_index(collection_key)
                # é‡è¯•ä¸€æ¬¡
                return self.collections[collection_key].query(**search_params)
            # å…¶å®ƒå¼‚å¸¸æŒ‰åŸæ ·æŠ›å‡º
            raise

    def _rebuild_collection_index(self, collection_key: str, batch_size: int = 512):
        """
        é‡å»ºæŒ‡å®šé›†åˆçš„ HNSW ç´¢å¼•ï¼š
        1) è¯»å–å½“å‰é›†åˆçš„æ•°æ®ï¼ˆids/documents/metadatas/embeddings-è‹¥å¯ç”¨ï¼‰
        2) åˆ é™¤é›†åˆå¹¶æŒ‰åŸé…ç½®é‡å»º
        3) åˆ†æ‰¹å†™å›æ•°æ®ï¼Œè§¦å‘ç´¢å¼•é‡å»ºï¼ˆè‹¥ embeddings å¯ç”¨åˆ™ç›´æ¥å†™å…¥ä»¥é¿å…é‡æ–°è®¡ç®—ï¼‰
        """
        try:
            coll = self.collections.get(collection_key)
            if coll is None:
                print(f"âŒ é‡å»ºå¤±è´¥ï¼šé›†åˆ '{collection_key}' ä¸å­˜åœ¨")
                return

            # è¯»å–æ•°æ®ï¼Œä¼˜å…ˆåŒ…å« embeddingsï¼›è‹¥ä¸æ”¯æŒåˆ™é™çº§
            data = None
            try:
                data = coll.get(include=["ids", "documents", "metadatas", "embeddings"])
            except Exception:
                data = coll.get(include=["ids", "documents", "metadatas"])  # æŸäº›å­˜å‚¨åç«¯å¯èƒ½ä¸æ”¯æŒ embeddings å¯¼å‡º

            ids = data.get("ids", []) or []
            docs = data.get("documents", []) or []
            metas = data.get("metadatas", []) or []
            embeds = data.get("embeddings") if isinstance(data, dict) else None

            total = len(ids)
            print(f"[MemorySystem] å¤‡ä»½é›†åˆ '{collection_key}' ä¸­çš„ {total} æ¡è®°å½•ç”¨äºé‡å»º")

            # åˆ é™¤å¹¶é‡å»ºé›†åˆ
            self.client.delete_collection(name=collection_key)
            new_coll = self.client.get_or_create_collection(
                name=collection_key,
                embedding_function=self.embedding_function,
                metadata=self.hnsw_metadata_config
            )
            self.collections[collection_key] = new_coll

            # è‹¥åŸé›†åˆä¸ºç©ºï¼Œç›´æ¥è¿”å›
            if total == 0:
                print(f"[MemorySystem] é›†åˆ '{collection_key}' ä¸ºç©ºï¼Œå·²å®Œæˆç©ºç´¢å¼•é‡å»º")
                return

            # åˆ†æ‰¹å†™å›
            has_embeds = isinstance(embeds, list) and len(embeds) == total
            for i in range(0, total, batch_size):
                batch_ids = ids[i:i+batch_size]
                batch_docs = docs[i:i+batch_size]
                batch_metas = metas[i:i+batch_size]
                if has_embeds:
                    batch_embeds = embeds[i:i+batch_size]
                    new_coll.add(
                        ids=batch_ids,
                        documents=batch_docs,
                        metadatas=batch_metas,
                        embeddings=batch_embeds
                    )
                else:
                    new_coll.add(
                        ids=batch_ids,
                        documents=batch_docs,
                        metadatas=batch_metas
                    )
            print(f"âœ… é›†åˆ '{collection_key}' çš„ HNSW ç´¢å¼•é‡å»ºå®Œæˆï¼Œå…±å†™å› {total} æ¡è®°å½•")
        except Exception as e:
            print(f"âŒ é‡å»ºé›†åˆ '{collection_key}' ç´¢å¼•å¤±è´¥: {e}")

    def _parse_iso_datetime(self, dt_str: str) -> Optional[datetime]:
        """å°½å¯èƒ½ç¨³å¥åœ°è§£æ ISO æ—¶é—´æˆ³ï¼Œè¿”å› UTC æ—¶åŒºçš„ datetimeã€‚
        å…¼å®¹ç¤ºä¾‹ï¼š
        - 2025-08-24T00:00:00Z
        - 2025-08-24T00:00:00.123Z
        - 2025-08-24T00:00:00+08:00
        - 2025-08-24 00:00:00  ï¼ˆæ— æ—¶åŒºï¼ŒæŒ‰ UTC å¤„ç†ï¼‰
        """
        if not isinstance(dt_str, str):
            return None
        s = dt_str.strip().replace(' ', 'T')
        # å…¼å®¹ä»¥ Z ç»“å°¾ï¼ˆUTCï¼‰
        if s.endswith('Z'):
            s = s[:-1] + '+00:00'
        try:
            dt = datetime.fromisoformat(s)
        except ValueError:
            return None

        # ç»Ÿä¸€è½¬ä¸º UTC æœ‰æ—¶åŒºæ—¶é—´
        if dt.tzinfo is None:
            # æ— æ—¶åŒºä¿¡æ¯æ—¶ï¼ŒæŒ‰ UTC å¤„ç†
            dt = dt.replace(tzinfo=timezone.utc)
        else:
            dt = dt.astimezone(timezone.utc)
        return dt

    # === ğŸ§â€â™‚ï¸ ç”¨æˆ·ç”»åƒ ===
    # æ›´æ–°ç­–ç•¥ï¼Œæ¯æ¬¡å¯¹è¯åéƒ½å¼‚æ­¥ä¿å­˜ç”¨æˆ·ç”»åƒï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹ï¼Œä¸”åœ¨æ™ºèƒ½ä½“ç©ºé—²æ—¶è°ƒç”¨æ¨¡å‹å¤„ç†åˆå¹¶é‡å¤å­—æ®µ
    def save_user_profile(self, profile: Dict):
        """ä¿å­˜ç”¨æˆ·ç”»åƒ"""
        profile["last_updated"] = get_timestamp()
        with open(PROFILE_PATH, "w", encoding="utf-8") as f:
            json.dump(profile, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç”¨æˆ·ç”»åƒå·²ä¿å­˜")

    def load_user_profile(self) -> Optional[Dict]:
        """åŠ è½½ç”¨æˆ·ç”»åƒ"""
        if not os.path.exists(PROFILE_PATH):
            return None
        with open(PROFILE_PATH, encoding="utf-8") as f:
            return json.load(f)

    def update_user_profile(self, **updates):
        """æ›´æ–°ç”¨æˆ·ç”»åƒï¼ˆéƒ¨åˆ†æ›´æ–°ï¼‰"""
        profile = self.load_user_profile() or {}
        profile.update(updates)
        self.save_user_profile(profile)

    # === ğŸ’¬ å¯¹è¯è®°å½•è®°å¿† ===
    def save_dialog_log(self, user_input: str, ai_response: str, topic: str, 
                       sentiment: str, importance: float, tags: List[str], 
                       additional_metadata: Optional[Dict] = None):
        """ä¿å­˜å¯¹è¯è®°å½•åˆ°ChromaDB"""
        dialog_id = f"dialog_{uuid4().hex}"
        timestamp = get_iso_timestamp()
        current_time = datetime.now()
        natural_time = format_natural_time(current_time)
        tags_str = "ã€".join(tags) if tags else "æ— "
        
        # å°†å…³é”®å…ƒæ•°æ®ä¿¡æ¯è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€
        document_header = f"[å¯¹è¯æ‘˜è¦] è¿™æ˜¯ä¸€æ®µè®°å½•äº {natural_time} çš„å¯¹è¯ã€‚å¯¹è¯ä¸»é¢˜æ˜¯â€œ{topic}â€ï¼Œæ•´ä½“æƒ…æ„ŸåŸºè°ƒä¸ºâ€œ{sentiment}â€ï¼Œç›¸å…³æ ‡ç­¾ä¸ºâ€œ{tags_str}â€ã€‚\n\n"
        dialog_body = f"[å¯¹è¯å†…å®¹]\nç”¨æˆ·ï¼š{user_input}\nAIï¼š{ai_response}"
        dialog_content = document_header + dialog_body
        
        #  metadata åªä¿ç•™ç”¨äºç²¾ç¡®è¿‡æ»¤çš„ç»“æ„åŒ–æ•°æ®
        metadata = {
            "type": "dialog_log",
            "timestamp": timestamp,
            "topic": topic,
            "sentiment": sentiment,
            "importance": importance,
            "tags": json.dumps(tags, ensure_ascii=False)
        }
        
        if additional_metadata:
            metadata.update(additional_metadata)
        
        try:
            self.collections["dialog_logs"].add(
                ids=[dialog_id],
                metadatas=[metadata],
                documents=[dialog_content]
            )
            print(f"âœ… å¯¹è¯è®°å½•å·²ä¿å­˜: {topic} (é‡è¦æ€§: {importance})")
            self.update_active_tags(tags)
            return dialog_id
        except Exception as e:
            print(f"âŒ ä¿å­˜å¯¹è¯è®°å½•å¤±è´¥: {e}")
            return None

    def search_dialog_logs(self, query: str, n_results: int = 5, 
                          where_filter: Optional[Dict] = None, 
                          threshold: float = 0.5) -> List[Dict]:
        """æœç´¢å¯¹è¯è®°å½•"""
        try:
            search_params = {
                "query_texts": [query],
                "n_results": n_results
            }
            
            if where_filter:
                search_params["where"] = where_filter
            
            # ä½¿ç”¨å®‰å…¨æŸ¥è¯¢ï¼Œå¿…è¦æ—¶è‡ªåŠ¨é‡å»ºç´¢å¼•
            results = self._safe_query("dialog_logs", search_params)
            
            dialog_memories = []
            if (results and results["documents"] and results["documents"][0] and
                results["metadatas"] and results["metadatas"][0] and
                results["distances"] and results["distances"][0]):
                
                docs_list = results["documents"][0]
                metadatas_list = results["metadatas"][0]
                distances_list = results["distances"][0]
                ids_list = results["ids"][0]
                
                for i, doc_content in enumerate(docs_list):
                    if i < len(distances_list):
                        distance = distances_list[i]
                        if distance <= threshold: 
                            metadata = metadatas_list[i]
                            # è§£ææ ‡ç­¾
                            tags = json.loads(metadata.get("tags", "[]"))
                            
                            dialog_memory = {
                                "id": ids_list[i],
                                "content": doc_content,
                                "metadata": metadata,
                                "tags": tags,
                                "similarity": 1 - distance,
                                "topic": metadata.get("topic", ""),
                                "sentiment": metadata.get("sentiment", ""),
                                "importance": metadata.get("importance", 0.0),
                                "timestamp": metadata.get("timestamp", "")
                            }
                            dialog_memories.append(dialog_memory)
            
            return dialog_memories
        except Exception as e:
            print(f"âŒ æœç´¢å¯¹è¯è®°å½•å¤±è´¥: {e}")
            return []

    def get_recent_dialogs(self, limit: int = 5) -> List[Dict]:
        """è·å–æœ€è¿‘çš„å¯¹è¯è®°å½•"""
        try:
            # è·å–æ‰€æœ‰å¯¹è¯è®°å½•
            all_dialogs = self.collections["dialog_logs"].get()
            
            if not all_dialogs or not all_dialogs["metadatas"]:
                return []
            
            # æŒ‰æ—¶é—´æˆ³æ’åº
            dialog_list = []
            for i, metadata in enumerate(all_dialogs["metadatas"]):
                dialog_info = {
                    "id": all_dialogs["ids"][i],
                    "content": all_dialogs["documents"][i],
                    "metadata": metadata,
                    "timestamp": metadata.get("timestamp", ""),
                    "tags": json.loads(metadata.get("tags", "[]"))
                }
                dialog_list.append(dialog_info)
            
            # æŒ‰æ—¶é—´æˆ³å€’åºæ’åº
            dialog_list.sort(key=lambda x: x["timestamp"], reverse=True)
            
            return dialog_list[:limit*2]
        except Exception as e:
            print(f"âŒ è·å–æœ€è¿‘å¯¹è¯å¤±è´¥: {e}")
            return []

    # === æ¦‚å¿µçŸ¥è¯†ï¼ˆäº‹å®è®°å¿†ï¼‰===
    # å…ˆå­˜åˆ°ç¼“å†²æ–‡ä»¶ï¼ˆæŒä¹…åŒ–ï¼‰ï¼Œç„¶ååœ¨ç©ºé—²æ—¶ç»è¿‡æ¨¡å‹å¤„ç†åä¿å­˜åˆ°ChromaDB
    def save_fact_memory(self, content: str, tags: List[str], 
                        source: str = "dialog", confidence: float = 1.0,
                        additional_metadata: Optional[Dict] = None):
        """ä¿å­˜äº‹å®è®°å¿†åˆ°ChromaDB"""
        fact_id = f"fact_{uuid4().hex}"
        timestamp = get_iso_timestamp()
        natural_time = format_natural_time(datetime.now())
        tags_str = "ã€".join(tags) if tags else "æ— "

        fact_content = f"[äº‹å®è®°å¿†] è¿™æ˜¯ä¸€æ¡è®°å½•äº {natural_time} çš„äº‹å®ï¼Œæ¥æºæ˜¯â€œ{source}â€ï¼Œç›¸å…³æ ‡ç­¾ä¸ºâ€œ{tags_str}â€ã€‚äº‹å®å†…å®¹ï¼š{content}"
        
        metadata = {
            "type": "fact",
            "timestamp": timestamp,
            "source": source,
            "confidence": confidence,
            "tags": json.dumps(tags, ensure_ascii=False)
        }
        
        if additional_metadata:
            metadata.update(additional_metadata)
        
        try:
            self.collections["facts"].add(
                ids=[fact_id],
                metadatas=[metadata],
                documents=[fact_content]
            )
            print(f"âœ… äº‹å®è®°å¿†å·²ä¿å­˜: {content[:30]}... (ç½®ä¿¡åº¦: {confidence})")
            self.update_active_tags(tags)
            return fact_id
        except Exception as e:
            print(f"âŒ ä¿å­˜äº‹å®è®°å¿†å¤±è´¥: {e}")
            return None

    def search_fact_memory(self, query: str, n_results: int = 3,
                          where_filter: Optional[Dict] = None,
                          threshold: float = 0.5) -> List[Dict]:
        """æœç´¢äº‹å®è®°å¿†"""
        try:
            search_params = {
                "query_texts": [query],
                "n_results": n_results
            }
            
            if where_filter:
                search_params["where"] = where_filter
            
            # ä½¿ç”¨å®‰å…¨æŸ¥è¯¢ï¼Œå¿…è¦æ—¶è‡ªåŠ¨é‡å»ºç´¢å¼•
            results = self._safe_query("facts", search_params)
            
            fact_memories = []
            if (results and results["documents"] and results["documents"][0] and
                results["metadatas"] and results["metadatas"][0] and
                results["distances"] and results["distances"][0]):
                
                docs_list = results["documents"][0]
                metadatas_list = results["metadatas"][0]
                distances_list = results["distances"][0]
                ids_list = results["ids"][0]
                
                for i, doc_content in enumerate(docs_list):
                    if i < len(distances_list):
                        distance = distances_list[i]
                        if distance <= threshold:
                            metadata = metadatas_list[i]
                            tags = json.loads(metadata.get("tags", "[]"))
                            
                            fact_memory = {
                                "id": ids_list[i],
                                "content": doc_content,
                                "metadata": metadata,
                                "tags": tags,
                                "similarity": 1 - distance,
                                "source": metadata.get("source", ""),
                                "confidence": metadata.get("confidence", 1.0),
                                "timestamp": metadata.get("timestamp", "")
                            }
                            fact_memories.append(fact_memory)
            
            return fact_memories
        except Exception as e:
            print(f"âŒ æœç´¢äº‹å®è®°å¿†å¤±è´¥: {e}")
            return []

    def update_fact_confidence(self, fact_id: str, new_confidence: float):
        """æ›´æ–°äº‹å®è®°å¿†çš„ç½®ä¿¡åº¦"""
        try:
            # è·å–ç°æœ‰è®°å¿†
            result = self.collections["facts"].get(ids=[fact_id])
            if result and result["metadatas"]:
                metadata = result["metadatas"][0]
                metadata["confidence"] = new_confidence
                metadata["last_updated"] = get_timestamp()
                
                # æ›´æ–°è®°å¿†
                self.collections["facts"].update(
                    ids=[fact_id],
                    metadatas=[metadata]
                )
                print(f"âœ… äº‹å®è®°å¿†ç½®ä¿¡åº¦å·²æ›´æ–°: {new_confidence}")
                return True
        except Exception as e:
            print(f"âŒ æ›´æ–°äº‹å®è®°å¿†ç½®ä¿¡åº¦å¤±è´¥: {e}")
        return False

    # === ç¼“å­˜ç®¡ç†æ–¹æ³• ===
    
    def cache_user_preference(self, content: str, preference_type: str, 
                             tags: List[str], confidence: float = 1.0):
        """ç¼“å­˜ç”¨æˆ·åå¥½ä¿¡æ¯åˆ°æœ¬åœ°JSONæ–‡ä»¶"""
        
        # åˆ›å»ºç¼“å­˜æ¡ç›®
        cache_entry = {
            "id": f"preference_cache_{uuid4().hex}",
            "content": content,
            "preference_type": preference_type,
            "tags": tags,
            "confidence": confidence,
            "timestamp": get_iso_timestamp(),
            "natural_time": format_natural_time(datetime.now())
        }
        
        # è¯»å–ç°æœ‰ç¼“å­˜
        preferences_cache = self._load_cache_file(PREFERENCE_CACHE_PATH)
        
        # æ·»åŠ æ–°æ¡ç›®
        preferences_cache.append(cache_entry)
        
        # ä¿å­˜ç¼“å­˜
        self._save_cache_file(PREFERENCE_CACHE_PATH, preferences_cache)
        
        print(f"âœ… ç”¨æˆ·åå¥½å·²ç¼“å­˜: {preference_type} - {content[:30]}...")
        return cache_entry["id"]
    
    def cache_fact_memory(self, content: str, tags: List[str], 
                         source: str = "dialog", confidence: float = 1.0):
        """ç¼“å­˜äº‹å®è®°å¿†åˆ°æœ¬åœ°JSONæ–‡ä»¶"""
        cache_entry = {
            "id": f"fact_cache_{uuid4().hex}",
            "content": content,
            "tags": tags,
            "source": source,
            "confidence": confidence,
            "timestamp": get_iso_timestamp(),
            "natural_time": format_natural_time(datetime.now()),
            "content_length": len(content)
        }
        
        facts_cache = self._load_cache_file(FACT_CACHE_PATH)
        facts_cache.append(cache_entry)
        self._save_cache_file(FACT_CACHE_PATH, facts_cache)
        
        print(f"âœ… äº‹å®è®°å¿†å·²ç¼“å­˜: {content[:30]}... (ç½®ä¿¡åº¦: {confidence})")
        return cache_entry["id"]
    
    def cache_profile_update(self, profile_data: Dict, source: str = "dialog"):
        """ç¼“å­˜ç”¨æˆ·ç”»åƒæ›´æ–°ä¿¡æ¯åˆ°æœ¬åœ°JSONæ–‡ä»¶"""
        
        # åˆ›å»ºç¼“å­˜æ¡ç›®
        cache_entry = {
            "id": f"profile_cache_{uuid4().hex}",
            "profile_data": profile_data,
            "source": source,
            "timestamp": get_iso_timestamp(),
            "natural_time": format_natural_time(datetime.now())
        }
        
        # è¯»å–ç°æœ‰ç¼“å­˜
        profile_cache = self._load_cache_file(PROFILE_CACHE_PATH)
        
        # æ·»åŠ æ–°æ¡ç›®
        profile_cache.append(cache_entry)
        
        # ä¿å­˜ç¼“å­˜
        self._save_cache_file(PROFILE_CACHE_PATH, profile_cache)
        
        print(f"âœ… ç”¨æˆ·ç”»åƒä¿¡æ¯å·²ç¼“å­˜: {list(profile_data.keys())}")
        return cache_entry["id"]
    
    def _load_cache_file(self, file_path: str) -> List[Dict]:
        """åŠ è½½ç¼“å­˜æ–‡ä»¶"""
        if not os.path.exists(file_path):
            return []
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶æŸåæˆ–ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶: {file_path}")
            return []
    
    def _save_cache_file(self, file_path: str, cache_data: List[Dict]):
        """ä¿å­˜ç¼“å­˜æ–‡ä»¶"""
        try:
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ä¿å­˜ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
    
    def load_preference_cache(self) -> List[Dict]:
        """è¯»å–ç”¨æˆ·åå¥½ç¼“å­˜"""
        return self._load_cache_file(PREFERENCE_CACHE_PATH)
    
    def load_fact_cache(self) -> List[Dict]:
        """è¯»å–äº‹å®è®°å¿†ç¼“å­˜"""
        return self._load_cache_file(FACT_CACHE_PATH)
    
    def load_profile_cache(self) -> List[Dict]:
        """è¯»å–ç”¨æˆ·ç”»åƒç¼“å­˜"""
        return self._load_cache_file(PROFILE_CACHE_PATH)
    
    def clear_preference_cache(self):
        """æ¸…ç©ºç”¨æˆ·åå¥½ç¼“å­˜"""
        self._save_cache_file(PREFERENCE_CACHE_PATH, [])
        print("ğŸ—‘ï¸ ç”¨æˆ·åå¥½ç¼“å­˜å·²æ¸…ç©º")
    
    def clear_fact_cache(self):
        """æ¸…ç©ºäº‹å®è®°å¿†ç¼“å­˜"""
        self._save_cache_file(FACT_CACHE_PATH, [])
        print("ï¿½ï¸ äº‹å®è®°å¿†ç¼“å­˜å·²æ¸…ç©º")
    
    def clear_profile_cache(self):
        """æ¸…ç©ºç”¨æˆ·ç”»åƒç¼“å­˜"""
        self._save_cache_file(PROFILE_CACHE_PATH, [])
        print("ğŸ—‘ï¸ ç”¨æˆ·ç”»åƒç¼“å­˜å·²æ¸…ç©º")
    
    def get_cache_status(self) -> Dict[str, int]:
        """è·å–å„ç¼“å­˜æ–‡ä»¶çš„çŠ¶æ€"""
        status = {
            "preferences_cache": len(self._load_cache_file(PREFERENCE_CACHE_PATH)),
            "facts_cache": len(self._load_cache_file(FACT_CACHE_PATH)),
            "profile_cache": len(self._load_cache_file(PROFILE_CACHE_PATH))
        }
        
        total = sum(status.values())
        print(f"ğŸ“Š ç¼“å­˜çŠ¶æ€: åå¥½ {status['preferences_cache']} æ¡ï¼Œäº‹å® {status['facts_cache']} æ¡ï¼Œç”»åƒ {status['profile_cache']} æ¡ï¼Œæ€»è®¡ {total} æ¡")
        
        return status
    
    def clear_all_caches(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        self.clear_preference_cache()
        self.clear_fact_cache()
        self.clear_profile_cache()
        print("ğŸ—‘ï¸ æ‰€æœ‰ç¼“å­˜å·²æ¸…ç©º")

    # === ç”¨æˆ·åå¥½ä¿¡æ¯ ===
    # å…ˆå­˜åˆ°ç¼“å†²æ–‡ä»¶ï¼ˆæŒä¹…åŒ–ï¼‰ï¼Œç„¶ååœ¨ç©ºé—²æ—¶ç»è¿‡æ¨¡å‹å¤„ç†åä¿å­˜åˆ°ChromaDB
    def save_user_preference(self, content: str, preference_type: str, 
                            tags: List[str], additional_metadata: Optional[Dict] = None):
        """ä¿å­˜ç”¨æˆ·åå¥½ä¿¡æ¯åˆ°ChromaDB"""
        preference_id = f"preference_{uuid4().hex}"
        timestamp = get_iso_timestamp()
        natural_time = format_natural_time(datetime.now())
        tags_str = "ã€".join(tags) if tags else "æ— "

        preference_content = f"[ç”¨æˆ·åå¥½] è¿™æ˜¯ä¸€æ¡è®°å½•äº {natural_time} çš„å…³äºç”¨æˆ·çš„åå¥½ä¿¡æ¯ï¼Œç±»å‹ä¸ºâ€œ{preference_type}â€ï¼Œç›¸å…³æ ‡ç­¾ä¸ºâ€œ{tags_str}â€ã€‚åå¥½å†…å®¹ï¼š{content}"
        
        metadata = {
            "type": "preference",
            "preference_type": preference_type,
            "tags": json.dumps(tags, ensure_ascii=False),
            "timestamp": timestamp
        }
        
        if additional_metadata:
            metadata.update(additional_metadata)
        
        try:
            self.collections["user_preferences"].add(
                ids=[preference_id],
                metadatas=[metadata],
                documents=[preference_content]
            )
            print(f"âœ… ç”¨æˆ·åå¥½å·²ä¿å­˜: {preference_type} - {content[:30]}...")
            self.update_active_tags(tags)
            return preference_id
        except Exception as e:
            print(f"âŒ ä¿å­˜ç”¨æˆ·åå¥½å¤±è´¥: {e}")
            return None

    def search_user_preferences(self, query: str, n_results: int = 5,
                               where_filter: Optional[Dict] = None,
                               threshold: float = 0.5) -> List[Dict]:
        """æœç´¢ç”¨æˆ·åå¥½ä¿¡æ¯"""
        try:
            search_params = {
                "query_texts": [query],
                "n_results": n_results
            }
            
            if where_filter:
                search_params["where"] = where_filter
            
            # ä½¿ç”¨å®‰å…¨æŸ¥è¯¢ï¼Œå¿…è¦æ—¶è‡ªåŠ¨é‡å»ºç´¢å¼•
            results = self._safe_query("user_preferences", search_params)
            
            preference_memories = []
            if (results and results["documents"] and results["documents"][0] and
                results["metadatas"] and results["metadatas"][0] and
                results["distances"] and results["distances"][0]):
                
                docs_list = results["documents"][0]
                metadatas_list = results["metadatas"][0]
                distances_list = results["distances"][0]
                ids_list = results["ids"][0]
                
                for i, doc_content in enumerate(docs_list):
                    if i < len(distances_list):
                        distance = distances_list[i]
                        if distance <= threshold:
                            metadata = metadatas_list[i]
                            tags = json.loads(metadata.get("tags", "[]"))
                            
                            preference_memory = {
                                "id": ids_list[i],
                                "content": doc_content,
                                "metadata": metadata,
                                "tags": tags,
                                "similarity": 1 - distance,
                                "type": metadata.get("type", ""),
                                "timestamp": metadata.get("timestamp", "")
                            }
                            preference_memories.append(preference_memory)
            
            return preference_memories
        except Exception as e:
            print(f"âŒ æœç´¢ç”¨æˆ·åå¥½å¤±è´¥: {e}")
            return []

    def get_preferences_by_type(self, preference_type: str, limit: int = 10) -> List[Dict]:
        """æ ¹æ®ç±»å‹è·å–ç”¨æˆ·åå¥½"""
        return self.search_user_preferences(
            query="", 
            n_results=limit,
            where_filter={"type": preference_type}
        )

    # === é‡å¤§äº‹ä»¶ç®¡ç† ===
    def save_important_event(self, content: str, event_type: str, summary: str,
                            tags: List[str], additional_metadata: Optional[Dict] = None):
        """ä¿å­˜é‡å¤§äº‹ä»¶åˆ°ChromaDB"""
        event_id = f"event_{uuid4().hex}"
        timestamp = get_iso_timestamp()
        natural_time = format_natural_time(datetime.now())
        tags_str = "ã€".join(tags) if tags else "æ— "

        event_content = f"[é‡å¤§äº‹ä»¶] è¿™æ˜¯ä¸€æ¡è®°å½•äº {natural_time} çš„é‡å¤§äº‹ä»¶ã€‚äº‹ä»¶ç±»å‹ä¸ºâ€œ{event_type}â€ï¼Œæ¦‚è¦æ˜¯â€œ{summary}â€ï¼Œç›¸å…³æ ‡ç­¾ä¸ºâ€œ{tags_str}â€ã€‚\n\n[è¯¦ç»†å†…å®¹]\n{content}"
        
        metadata = {
            "type": "important_event",
            "event_type": event_type,
            "summary": summary,
            "tags": json.dumps(tags, ensure_ascii=False),
            "timestamp": timestamp
        }
        
        if additional_metadata:
            metadata.update(additional_metadata)
        
        try:
            self.collections["important_events"].add(
                ids=[event_id],
                metadatas=[metadata],
                documents=[event_content]
            )
            print(f"âœ… é‡å¤§äº‹ä»¶å·²ä¿å­˜: {event_type} - {summary}")
            self.update_active_tags(tags)
            return event_id
        except Exception as e:
            print(f"âŒ ä¿å­˜é‡å¤§äº‹ä»¶å¤±è´¥: {e}")
            return None

    def search_important_events(self, query: str, n_results: int = 5,
                               where_filter: Optional[Dict] = None,
                               threshold: float = 0.5) -> List[Dict]:
        """æœç´¢é‡å¤§äº‹ä»¶"""
        try:
            search_params = {
                "query_texts": [query],
                "n_results": n_results
            }
            
            if where_filter:
                search_params["where"] = where_filter
            
            # ä½¿ç”¨å®‰å…¨æŸ¥è¯¢ï¼Œå¿…è¦æ—¶è‡ªåŠ¨é‡å»ºç´¢å¼•
            results = self._safe_query("important_events", search_params)
            
            event_memories = []
            if (results and results["documents"] and results["documents"][0] and
                results["metadatas"] and results["metadatas"][0] and
                results["distances"] and results["distances"][0]):
                
                docs_list = results["documents"][0]
                metadatas_list = results["metadatas"][0]
                distances_list = results["distances"][0]
                ids_list = results["ids"][0]
                
                for i, doc_content in enumerate(docs_list):
                    if i < len(distances_list):
                        distance = distances_list[i]
                        if distance <= threshold:
                            metadata = metadatas_list[i]
                            tags = json.loads(metadata.get("tags", "[]"))
                            
                            event_memory = {
                                "id": ids_list[i],
                                "content": doc_content,
                                "metadata": metadata,
                                "tags": tags,
                                "similarity": 1 - distance,
                                "event_type": metadata.get("event_type", ""),
                                "summary": metadata.get("summary", ""),
                                "timestamp": metadata.get("timestamp", "")
                            }
                            event_memories.append(event_memory)
            
            return event_memories
        except Exception as e:
            print(f"âŒ æœç´¢é‡å¤§äº‹ä»¶å¤±è´¥: {e}")
            return []

    def get_events_by_type(self, event_type: str, limit: int = 10) -> List[Dict]:
        """æ ¹æ®ç±»å‹è·å–é‡å¤§äº‹ä»¶"""
        return self.search_important_events(
            query="", 
            n_results=limit,
            where_filter={"event_type": event_type}
        )

    # === â° è¿‘æœŸå…³æ³¨äº‹ä»¶ç®¡ç† ===
    def save_temp_focus_event(self, content: str, event_time: str, 
                             expire_time: str, tags: List[str]):
        """ä¿å­˜è¿‘æœŸå…³æ³¨äº‹ä»¶"""
        temp_event = {
            "id": f"temp_{uuid4().hex}",
            "created_at": get_iso_timestamp(),
            "event_time": event_time,
            "expire_time": expire_time,
            "content": content,
            "tags": tags
        }
        
        # è¯»å–ç°æœ‰äº‹ä»¶
        temp_events = self.load_temp_focus_events()
        
        # æ·»åŠ æ–°äº‹ä»¶
        temp_events.append(temp_event)
        
        # ä¿å­˜æ›´æ–°åçš„äº‹ä»¶åˆ—è¡¨
        try:
            with open(TEMP_FOCUS_EVENTS_PATH, "w", encoding="utf-8") as f:
                json.dump(temp_events, f, ensure_ascii=False, indent=2)
            print(f"âœ… è¿‘æœŸå…³æ³¨äº‹ä»¶å·²ä¿å­˜: {content[:30]}...")
            
            # æ›´æ–°æ´»è·ƒæ ‡ç­¾
            self.update_active_tags(tags)
            
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜è¿‘æœŸå…³æ³¨äº‹ä»¶å¤±è´¥: {e}")
            return False

    def load_temp_focus_events(self) -> List[Dict]:
        """åŠ è½½è¿‘æœŸå…³æ³¨äº‹ä»¶ï¼ˆè‡ªåŠ¨æ¸…ç†è¿‡æœŸäº‹ä»¶ï¼‰"""
        if not os.path.exists(TEMP_FOCUS_EVENTS_PATH):
            return []
        
        try:
            with open(TEMP_FOCUS_EVENTS_PATH, encoding="utf-8") as f:
                events = json.load(f)
            # å…¼å®¹å†å²æ•°æ®ï¼šä¸ºç¼ºå°‘ id çš„äº‹ä»¶è¡¥é½å¹¶è½ç›˜
            updated_for_ids = False
            for e in events:
                if "id" not in e or not e.get("id"):
                    e["id"] = f"temp_{uuid4().hex}"
                    updated_for_ids = True
            if updated_for_ids:
                with open(TEMP_FOCUS_EVENTS_PATH, "w", encoding="utf-8") as f:
                    json.dump(events, f, ensure_ascii=False, indent=2)
            
            # è¿‡æ»¤æ‰è¿‡æœŸäº‹ä»¶ï¼ˆç»Ÿä¸€ä½¿ç”¨ UTC æ¯”è¾ƒï¼‰
            now_utc = datetime.now(timezone.utc)
            valid_events = []
            
            for event in events:
                expire_raw = event.get("expire_time")
                expire_dt = self._parse_iso_datetime(expire_raw)
                if expire_dt is None:
                    # è§£æå¤±è´¥åˆ™ä¿ç•™ï¼Œé¿å…è¯¯åˆ 
                    valid_events.append(event)
                else:
                    if now_utc < expire_dt:
                        valid_events.append(event)
                    # å¦åˆ™ä¸¢å¼ƒï¼ˆå·²è¿‡æœŸï¼‰

            # å¦‚æœæœ‰äº‹ä»¶è¢«æ¸…ç†ï¼Œæ›´æ–°æ–‡ä»¶
            if len(valid_events) != len(events):
                with open(TEMP_FOCUS_EVENTS_PATH, "w", encoding="utf-8") as f:
                    json.dump(valid_events, f, ensure_ascii=False, indent=2)
                print(f"ğŸ§¹ å·²æ¸…ç† {len(events) - len(valid_events)} ä¸ªè¿‡æœŸçš„å…³æ³¨äº‹ä»¶")
            
            return valid_events
        except Exception as e:
            print(f"âŒ åŠ è½½è¿‘æœŸå…³æ³¨äº‹ä»¶å¤±è´¥: {e}")
            return []

    def update_temp_focus_event_expire_time(self, event_index: int, new_expire_time: str):
        """æ›´æ–°è¿‘æœŸå…³æ³¨äº‹ä»¶çš„è¿‡æœŸæ—¶é—´"""
        events = self.load_temp_focus_events()
        
        if 0 <= event_index < len(events):
            events[event_index]["expire_time"] = new_expire_time
            
            try:
                with open(TEMP_FOCUS_EVENTS_PATH, "w", encoding="utf-8") as f:
                    json.dump(events, f, ensure_ascii=False, indent=2)
                print(f"âœ… äº‹ä»¶è¿‡æœŸæ—¶é—´å·²æ›´æ–°: {new_expire_time}")
                return True
            except Exception as e:
                print(f"âŒ æ›´æ–°äº‹ä»¶è¿‡æœŸæ—¶é—´å¤±è´¥: {e}")
        else:
            print(f"âŒ äº‹ä»¶ç´¢å¼• {event_index} è¶…å‡ºèŒƒå›´")
        
        return False

    def get_active_focus_events(self) -> List[Dict]:
        """è·å–å½“å‰æœ‰æ•ˆçš„å…³æ³¨äº‹ä»¶"""
        return self.load_temp_focus_events()

    def clear_expired_focus_events(self):
        """æ‰‹åŠ¨æ¸…ç†è¿‡æœŸçš„å…³æ³¨äº‹ä»¶"""
        # æ ¹æ®è¿‡æœŸæœºåˆ¶è‡ªåŠ¨æ¸…ç†
        valid_events = self.load_temp_focus_events()
        return len(valid_events)

    def delete_temp_focus_events_by_ids(self, ids: List[str]) -> int:
        """æŒ‰ ID åˆ é™¤ä¸´æ—¶å…³æ³¨äº‹ä»¶ï¼Œè¿”å›åˆ é™¤æ•°é‡ã€‚"""
        if not ids:
            return 0
        try:
            events = self.load_temp_focus_events()
            before = len(events)
            id_set = set(ids)
            remaining = [e for e in events if e.get("id") not in id_set]
            if len(remaining) != before:
                with open(TEMP_FOCUS_EVENTS_PATH, "w", encoding="utf-8") as f:
                    json.dump(remaining, f, ensure_ascii=False, indent=2)
                removed = before - len(remaining)
                print(f"ğŸ§¹ å·²æŒ‰IDåˆ é™¤ {removed} æ¡ä¸´æ—¶å…³æ³¨äº‹ä»¶")
                return removed
            return 0
        except Exception as e:
            print(f"âŒ æŒ‰IDåˆ é™¤ä¸´æ—¶å…³æ³¨äº‹ä»¶å¤±è´¥: {e}")
            return 0

    # === ğŸ”– æ´»è·ƒæ ‡ç­¾ ===
    def update_active_tags(self, new_tags: List[str]):
        """æ›´æ–°æ´»è·ƒæ ‡ç­¾ç»Ÿè®¡"""
        tags = {}
        if os.path.exists(ACTIVE_TAGS_PATH):
            try:
                with open(ACTIVE_TAGS_PATH, encoding="utf-8") as f:
                    data = json.load(f)
                    tags = data.get("tags", {})
            except:
                tags = {}
        
        # æ›´æ–°æ ‡ç­¾è®¡æ•°
        for tag in new_tags:
            tags[tag] = tags.get(tag, 0) + 1
        
        # ä¿å­˜æ ‡ç­¾æ•°æ®
        data = {
            "type": "active_tags", 
            "tags": tags, 
            "last_update": get_timestamp(),
            "total_tags": len(tags),
            "most_frequent": max(tags.items(), key=lambda x: x[1]) if tags else None
        }
        
        with open(ACTIVE_TAGS_PATH, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def get_active_tags(self, top_n: int = 10) -> Dict:
        """è·å–æœ€æ´»è·ƒçš„æ ‡ç­¾"""
        if not os.path.exists(ACTIVE_TAGS_PATH):
            return {"tags": {}, "top_tags": []}
        
        with open(ACTIVE_TAGS_PATH, encoding="utf-8") as f:
            data = json.load(f)
            tags = data.get("tags", {})
            
            # æŒ‰é¢‘ç‡æ’åº
            sorted_tags = sorted(tags.items(), key=lambda x: x[1], reverse=True)
            top_tags = sorted_tags[:top_n]
            
            return {
                "tags": tags,
                "top_tags": top_tags,
                "total_count": sum(tags.values()),
                "unique_count": len(tags),
                "last_update": data.get("last_update", "")
            }

    # === ğŸ” ç»¼åˆæœç´¢åŠŸèƒ½ ===
    def comprehensive_search(self, query: str, search_dialogs: bool = True, 
                           search_facts: bool = True, search_preferences: bool = True,
                           search_events: bool = True, n_results: int = 5) -> Dict:
        """ç»¼åˆæœç´¢æ‰€æœ‰ç±»å‹çš„è®°å¿†"""
        results = {
            "query": query,
            "timestamp": get_timestamp(),
            "dialog_memories": [],
            "fact_memories": [],
            "preference_memories": [],
            "event_memories": [],
            "focus_events": []
        }
        
        if search_dialogs:
            results["dialog_memories"] = self.search_dialog_logs(query, n_results)
        
        if search_facts:
            results["fact_memories"] = self.search_fact_memory(query, n_results)
            
        if search_preferences:
            results["preference_memories"] = self.search_user_preferences(query, n_results)
            
        if search_events:
            results["event_memories"] = self.search_important_events(query, n_results)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³çš„å…³æ³¨äº‹ä»¶
        focus_events = self.get_active_focus_events()
        relevant_focus_events = []
        for event in focus_events:
            if (query.lower() in event["content"].lower() or 
                any(tag.lower() in query.lower() for tag in event["tags"])):
                relevant_focus_events.append(event)
        results["focus_events"] = relevant_focus_events
        
        return results

    # === ğŸ“Š ç»Ÿè®¡å’Œç®¡ç†åŠŸèƒ½ ===
    def get_memory_statistics(self) -> Dict:
        """è·å–è®°å¿†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "timestamp": get_timestamp(),
            "dialog_count": 0,
            "fact_count": 0,
            "preference_count": 0,
            "event_count": 0,
            "focus_event_count": 0,
            "user_profile_exists": os.path.exists(PROFILE_PATH),
            "active_tags": self.get_active_tags(5)
        }
        
        try:
            # ç»Ÿè®¡å¯¹è¯è®°å½•æ•°é‡
            dialogs = self.collections["dialog_logs"].count()
            stats["dialog_count"] = dialogs
        except:
            pass
        
        try:
            # ç»Ÿè®¡äº‹å®è®°å¿†æ•°é‡
            facts = self.collections["facts"].count()
            stats["fact_count"] = facts
        except:
            pass
            
        try:
            # ç»Ÿè®¡ç”¨æˆ·åå¥½æ•°é‡
            preferences = self.collections["user_preferences"].count()
            stats["preference_count"] = preferences
        except:
            pass
            
        try:
            # ç»Ÿè®¡é‡å¤§äº‹ä»¶æ•°é‡
            events = self.collections["important_events"].count()
            stats["event_count"] = events
        except:
            pass
            
        # ç»Ÿè®¡å…³æ³¨äº‹ä»¶æ•°é‡
        focus_events = self.get_active_focus_events()
        stats["focus_event_count"] = len(focus_events)
        
        return stats

    # TODO æœªå®ç°åŠŸèƒ½ï¼Œæœªæ¥è€ƒè™‘å¼€å‘
    def cleanup_old_memories(self, days_threshold: int = 30, 
                           importance_threshold: float = 0.3):
        """æ¸…ç†æ—§çš„ä½é‡è¦æ€§è®°å¿†ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰"""
        print(f"ğŸ§¹ å¼€å§‹æ¸…ç† {days_threshold} å¤©å‰é‡è¦æ€§ä½äº {importance_threshold} çš„è®°å¿†...")
        
        from datetime import timedelta
        cutoff_date = (datetime.now() - timedelta(days=days_threshold)).isoformat()
        

# === ğŸ§© å…¨å±€å®ä¾‹å’Œä¾¿æ·å‡½æ•° ===

# åˆ›å»ºå…¨å±€è®°å¿†ç³»ç»Ÿå®ä¾‹
memory_system = MemorySystem()

# ä¾¿æ·å‡½æ•°
def save_dialog_log(user_input: str, ai_response: str, topic: str, 
                   sentiment: str, importance: float, tags: List[str]):
    """ä¾¿æ·çš„å¯¹è¯è®°å½•ä¿å­˜å‡½æ•°"""
    return memory_system.save_dialog_log(user_input, ai_response, topic, 
                                       sentiment, importance, tags)

def save_fact_memory(content: str, tags: List[str], source: str = "dialog"):
    """ä¾¿æ·çš„äº‹å®è®°å¿†ä¿å­˜å‡½æ•°"""
    return memory_system.save_fact_memory(content, tags, source)

def save_user_preference(content: str, preference_type: str, tags: List[str]):
    """ä¾¿æ·çš„ç”¨æˆ·åå¥½ä¿å­˜å‡½æ•°"""
    return memory_system.save_user_preference(content, preference_type, tags)

def save_important_event(content: str, event_type: str, summary: str, tags: List[str]):
    """ä¾¿æ·çš„é‡å¤§äº‹ä»¶ä¿å­˜å‡½æ•°"""
    return memory_system.save_important_event(content, event_type, summary, tags)

def save_temp_focus_event(content: str, event_time: str, expire_time: str, tags: List[str]):
    """ä¾¿æ·çš„ä¸´æ—¶å…³æ³¨äº‹ä»¶ä¿å­˜å‡½æ•°"""
    return memory_system.save_temp_focus_event(content, event_time, expire_time, tags)

def search_memories(query: str, n_results: int = 5):
    """ä¾¿æ·çš„è®°å¿†æœç´¢å‡½æ•°"""
    return memory_system.comprehensive_search(query, n_results=n_results)

# === ğŸ¯ å…¨å±€ä¾¿æ·å‡½æ•°ï¼ˆç¼“å­˜ç‰ˆæœ¬ï¼‰===

# å®ä¾‹åŒ–å…¨å±€è®°å¿†ç³»ç»Ÿ
_global_memory_system = None

def get_memory_system():
    """è·å–å…¨å±€è®°å¿†ç³»ç»Ÿå®ä¾‹"""
    global _global_memory_system
    if _global_memory_system is None:
        _global_memory_system = MemorySystem()
    return _global_memory_system

# ç¼“å­˜ç›¸å…³ä¾¿æ·å‡½æ•°
def cache_user_preference(content: str, preference_type: str, tags: List[str], 
                         confidence: float = 1.0):
    """ä¾¿æ·å‡½æ•°ï¼šç¼“å­˜ç”¨æˆ·åå¥½ä¿¡æ¯"""
    memory_system = get_memory_system()
    # ç°åœ¨çš„è°ƒç”¨æ˜¯æ­£ç¡®çš„ï¼Œå‚æ•°æ•°é‡å’Œç±»å‹éƒ½åŒ¹é…
    return memory_system.cache_user_preference(content, preference_type, tags, confidence)

def cache_fact_memory(content: str, tags: List[str], source: str = "dialog", 
                     confidence: float = 1.0): # <-- ç§»é™¤ additional_metadata
    """ä¾¿æ·å‡½æ•°ï¼šç¼“å­˜äº‹å®è®°å¿†"""
    memory_system = get_memory_system()
    # ç°åœ¨çš„è°ƒç”¨æ˜¯æ­£ç¡®çš„
    return memory_system.cache_fact_memory(content, tags, source, confidence)

def cache_profile_update(profile_data: Dict, source: str = "dialog"):
    """ä¾¿æ·å‡½æ•°ï¼šç¼“å­˜ç”¨æˆ·ç”»åƒæ›´æ–°"""
    memory_system = get_memory_system()
    return memory_system.cache_profile_update(profile_data, source)

def load_preference_cache():
    """ä¾¿æ·å‡½æ•°ï¼šè¯»å–ç”¨æˆ·åå¥½ç¼“å­˜"""
    memory_system = get_memory_system()
    return memory_system.load_preference_cache()

def load_fact_cache():
    """ä¾¿æ·å‡½æ•°ï¼šè¯»å–äº‹å®è®°å¿†ç¼“å­˜"""
    memory_system = get_memory_system()
    return memory_system.load_fact_cache()

def load_profile_cache():
    """ä¾¿æ·å‡½æ•°ï¼šè¯»å–ç”¨æˆ·ç”»åƒç¼“å­˜"""
    memory_system = get_memory_system()
    return memory_system.load_profile_cache()

def clear_preference_cache():
    """ä¾¿æ·å‡½æ•°ï¼šæ¸…ç©ºç”¨æˆ·åå¥½ç¼“å­˜"""
    memory_system = get_memory_system()
    return memory_system.clear_preference_cache()

def clear_fact_cache():
    """ä¾¿æ·å‡½æ•°ï¼šæ¸…ç©ºäº‹å®è®°å¿†ç¼“å­˜"""
    memory_system = get_memory_system()
    return memory_system.clear_fact_cache()

def clear_profile_cache():
    """ä¾¿æ·å‡½æ•°ï¼šæ¸…ç©ºç”¨æˆ·ç”»åƒç¼“å­˜"""
    memory_system = get_memory_system()
    return memory_system.clear_profile_cache()

def get_cache_status():
    """ä¾¿æ·å‡½æ•°ï¼šè·å–ç¼“å­˜çŠ¶æ€"""
    memory_system = get_memory_system()
    return memory_system.get_cache_status()

def clear_all_caches():
    """ä¾¿æ·å‡½æ•°ï¼šæ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
    memory_system = get_memory_system()
    return memory_system.clear_all_caches()
