# å·²ç»æ¥å…¥äº†å¯¹è¯é“¾ï¼Œä¸»è¦è´Ÿè´£ç”¨æˆ·ç”»åƒã€å¯¹è¯è®°å¿†ã€äº‹å®è®°å¿†ç­‰çš„å­˜å‚¨å’Œæ£€ç´¢ï¼Œåç»­éœ€è¦åœ¨å¯¹è¯å®Œåçš„æ­¥éª¤ä¸­æ›´æ–°è®°å¿†å†…å®¹ï¼šå¯¹è¯è®°å¿†ã€äº‹å®è®°å¿†ã€ç”¨æˆ·åå¥½ã€é‡å¤§äº‹ä»¶ç­‰ã€‚

import json
import os
# åœ¨å¯¼å…¥ä»»ä½•æ¨¡å‹å‰è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_DATASETS_OFFLINE"] = "1"

import chromadb
from datetime import datetime
from typing import List, Dict, Optional
from uuid import uuid4
from chromadb.utils import embedding_functions

# === ğŸ§  ä¸€ã€é€šç”¨ç»“æ„å®šä¹‰ ===

def get_timestamp():
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")  # å¯è¯»æ—¶é—´æˆ³

def get_iso_timestamp():
    return datetime.now().isoformat()  # ISOæ ¼å¼æ—¶é—´æˆ³ï¼Œç”¨äºChromaDB

def format_natural_time(dt: datetime) -> str:
    """å°†æ—¶é—´æ ¼å¼åŒ–ä¸ºè‡ªç„¶è¯­è¨€å½¢å¼"""
    weekdays = ['æ˜ŸæœŸä¸€', 'æ˜ŸæœŸäºŒ', 'æ˜ŸæœŸä¸‰', 'æ˜ŸæœŸå››', 'æ˜ŸæœŸäº”', 'æ˜ŸæœŸå…­', 'æ˜ŸæœŸæ—¥']
    weekday = weekdays[dt.weekday()]
    
    # åˆ¤æ–­æ—¶é—´æ®µ
    hour = dt.hour
    if 5 <= hour < 12:
        time_period = "ä¸Šåˆ"
    elif 12 <= hour < 14:
        time_period = "ä¸­åˆ"
    elif 14 <= hour < 18:
        time_period = "ä¸‹åˆ"
    elif 18 <= hour < 22:
        time_period = "æ™šä¸Š"
    else:
        time_period = "æ·±å¤œ"
    
    return f"{dt.year}å¹´{dt.month}æœˆ{dt.day}æ—¥{weekday}{time_period}"

# === ğŸ“¦ äºŒã€å­˜å‚¨ç›®å½•è®¾ç½® ===
# ä½¿ç”¨åŸºäº __file__ çš„å¥å£®è·¯å¾„æ„å»ºæ–¹æ³•
# 1. è·å–å½“å‰æ–‡ä»¶(memory_system.py)æ‰€åœ¨çš„ç›®å½•: .../src/MiraMate/modules/
MODULE_DIR = os.path.dirname(os.path.abspath(__file__))
# 2. ä»æ¨¡å—ç›®å½•å›æº¯ä¸‰å±‚ï¼Œåˆ°è¾¾é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.abspath(os.path.join(MODULE_DIR, '..', '..', '..'))
BASE_DIR = os.path.join(PROJECT_ROOT, "memory", "memory_storage")
PROFILE_PATH = os.path.join(BASE_DIR, "user_profile.json")
ACTIVE_TAGS_PATH = os.path.join(BASE_DIR, "active_tags.json")
TEMP_FOCUS_EVENTS_PATH = os.path.join(BASE_DIR, "temp_focus_events.json")

# ç¼“å­˜æ–‡ä»¶è·¯å¾„
PREFERENCE_CACHE_PATH = os.path.join(BASE_DIR, "preference_cache.json")
FACT_CACHE_PATH = os.path.join(BASE_DIR, "fact_cache.json")
PROFILE_CACHE_PATH = os.path.join(BASE_DIR, "profile_cache.json")

# ChromaDB å­˜å‚¨ç›®å½•
CHROMA_DB_DIR = os.path.join(BASE_DIR, "chroma_db")

# åˆ›å»ºå¿…è¦çš„ç›®å½•
os.makedirs(BASE_DIR, exist_ok=True)
os.makedirs(CHROMA_DB_DIR, exist_ok=True)

# === ğŸ”§ ä¸‰ã€ChromaDB åˆå§‹åŒ– ===

class MemorySystem:
    def __init__(self, persist_directory=None):
        """åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ"""
        if persist_directory is None:
            persist_directory = CHROMA_DB_DIR
            
        # å¤„ç†ç›¸å¯¹è·¯å¾„ï¼Œæ”¯æŒå®¹å™¨ç¯å¢ƒ
        if not os.path.isabs(persist_directory):
            if os.getenv('DOCKER_ENV'):
                # Dockerç¯å¢ƒä¸­ä½¿ç”¨ç»å¯¹è·¯å¾„
                persist_directory = f"/app/{persist_directory}"
            else:
                # æœ¬åœ°ç¯å¢ƒä¸­è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                current_dir = os.path.dirname(os.path.abspath(__file__))
                project_root = os.path.dirname(current_dir)  # å‡è®¾æ­¤æ–‡ä»¶åœ¨é¡¹ç›®æ ¹ç›®å½•
                persist_directory = os.path.join(project_root, persist_directory)
        
        # åˆ›å»ºæŒä¹…åŒ–ç›®å½•
        os.makedirs(persist_directory, exist_ok=True)
        
        # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯
        self.client = chromadb.PersistentClient(path=persist_directory)
        print(f"âœ… ChromaDBå®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼ŒæŒä¹…åŒ–ç›®å½•: {persist_directory}")

        # é…ç½®åµŒå…¥å‡½æ•°
        self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="BAAI/bge-base-zh-v1.5",
            device="cpu"
        )

        # å®šä¹‰HNSWç´¢å¼•å‚æ•°
        self.hnsw_metadata_config = {
            "hnsw:space": "cosine",          # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            "hnsw:M": 32,                    # æ¨èMå€¼
            "hnsw:construction_ef": 256,     # construction_efå€¼
            "hnsw:num_threads": 4            # æ„å»ºçº¿ç¨‹æ•°
        }

        # åˆå§‹åŒ–é›†åˆ
        self.collections = {
            "dialog_logs": self.client.get_or_create_collection(
                name="dialog_logs",
                embedding_function=self.embedding_function,
                metadata=self.hnsw_metadata_config
            ),
            "facts": self.client.get_or_create_collection(
                name="facts",
                embedding_function=self.embedding_function,
                metadata=self.hnsw_metadata_config
            ),
            "user_preferences": self.client.get_or_create_collection(
                name="user_preferences",
                embedding_function=self.embedding_function,
                metadata=self.hnsw_metadata_config
            ),
            "important_events": self.client.get_or_create_collection(
                name="important_events",
                embedding_function=self.embedding_function,
                metadata=self.hnsw_metadata_config
            )
        }

    # === ğŸ§â€â™‚ï¸ ç”¨æˆ·ç”»åƒ ===
    # æ›´æ–°ç­–ç•¥ï¼Œæ¯æ¬¡å¯¹è¯åéƒ½å¼‚æ­¥ä¿å­˜ç”¨æˆ·ç”»åƒï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹ï¼Œä¸”åœ¨æ™ºèƒ½ä½“ç©ºé—²æ—¶è°ƒç”¨æ¨¡å‹å¤„ç†åˆå¹¶é‡å¤å­—æ®µ
    def save_user_profile(self, profile: Dict):
        """ä¿å­˜ç”¨æˆ·ç”»åƒ"""
        profile["last_updated"] = get_timestamp()
        with open(PROFILE_PATH, "w", encoding="utf-8") as f:
            json.dump(profile, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç”¨æˆ·ç”»åƒå·²ä¿å­˜")

    def load_user_profile(self) -> Optional[Dict]:
        """åŠ è½½ç”¨æˆ·ç”»åƒ"""
        if not os.path.exists(PROFILE_PATH):
            return None
        with open(PROFILE_PATH, encoding="utf-8") as f:
            return json.load(f)

    def update_user_profile(self, **updates):
        """æ›´æ–°ç”¨æˆ·ç”»åƒï¼ˆéƒ¨åˆ†æ›´æ–°ï¼‰"""
        profile = self.load_user_profile() or {}
        profile.update(updates)
        self.save_user_profile(profile)

    # === ğŸ’¬ å¯¹è¯è®°å½•è®°å¿† ===
    # ä¿å­˜æ‰€æœ‰çš„å¯¹è¯è®°å¿†
    def save_dialog_log(self, user_input: str, ai_response: str, topic: str, 
                       sentiment: str, importance: float, tags: List[str], 
                       additional_metadata: Optional[Dict] = None):
        """ä¿å­˜å¯¹è¯è®°å½•åˆ°ChromaDB"""
        
        # ç”Ÿæˆå”¯ä¸€ID
        dialog_id = f"dialog_{uuid4().hex}"
        timestamp = get_iso_timestamp()
        current_time = datetime.now()
        natural_time = format_natural_time(current_time)
        
        # æ„å»ºåŒ…å«å®Œæ•´ä¿¡æ¯çš„æ–‡æ¡£å†…å®¹
        tags_str = "ã€".join(tags) if tags else "æ— æ ‡ç­¾"
        
        dialog_content = f"""æ—¶é—´ï¼š{natural_time}
è¯é¢˜ï¼š{topic}
æƒ…æ„Ÿï¼š{sentiment}
æ ‡ç­¾ï¼š{tags_str}
ç”¨æˆ·ï¼š{user_input}
AIï¼š{ai_response}"""
        
        # å‡†å¤‡å…ƒæ•°æ®ï¼ˆåªä¿ç•™å¿…è¦çš„ç»“æ„åŒ–æ•°æ®ï¼‰
        metadata = {
            "type": "dialog_log",
            "timestamp": timestamp,
            "topic": topic,
            "sentiment": sentiment,
            "importance": importance,
            "tags": json.dumps(tags, ensure_ascii=False),
            "user_input_length": len(user_input),
            "ai_response_length": len(ai_response)
        }
        
        # æ·»åŠ é¢å¤–å…ƒæ•°æ®
        if additional_metadata:
            metadata.update(additional_metadata)
        
        # ä¿å­˜åˆ°ChromaDB
        try:
            self.collections["dialog_logs"].add(
                ids=[dialog_id],
                metadatas=[metadata],
                documents=[dialog_content]
            )
            print(f"âœ… å¯¹è¯è®°å½•å·²ä¿å­˜: {topic} (é‡è¦æ€§: {importance})")
            
            # æ›´æ–°æ´»è·ƒæ ‡ç­¾
            self.update_active_tags(tags)
            
            return dialog_id
        except Exception as e:
            print(f"âŒ ä¿å­˜å¯¹è¯è®°å½•å¤±è´¥: {e}")
            return None

    def search_dialog_logs(self, query: str, n_results: int = 5, 
                          where_filter: Optional[Dict] = None, 
                          similarity_threshold: float = 0.6) -> List[Dict]:
        """æœç´¢å¯¹è¯è®°å½•"""
        try:
            search_params = {
                "query_texts": [query],
                "n_results": n_results
            }
            
            if where_filter:
                search_params["where"] = where_filter
            
            results = self.collections["dialog_logs"].query(**search_params)
            
            dialog_memories = []
            if (results and results["documents"] and results["documents"][0] and
                results["metadatas"] and results["metadatas"][0] and
                results["distances"] and results["distances"][0]):
                
                docs_list = results["documents"][0]
                metadatas_list = results["metadatas"][0]
                distances_list = results["distances"][0]
                ids_list = results["ids"][0]
                
                for i, doc_content in enumerate(docs_list):
                    if i < len(distances_list):
                        distance = distances_list[i]
                        if distance <= (1 - similarity_threshold):  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦é˜ˆå€¼
                            metadata = metadatas_list[i]
                            # è§£ææ ‡ç­¾
                            tags = json.loads(metadata.get("tags", "[]"))
                            
                            dialog_memory = {
                                "id": ids_list[i],
                                "content": doc_content,
                                "metadata": metadata,
                                "tags": tags,
                                "similarity": 1 - distance,
                                "topic": metadata.get("topic", ""),
                                "sentiment": metadata.get("sentiment", ""),
                                "importance": metadata.get("importance", 0.0),
                                "timestamp": metadata.get("timestamp", "")
                            }
                            dialog_memories.append(dialog_memory)
            
            return dialog_memories
        except Exception as e:
            print(f"âŒ æœç´¢å¯¹è¯è®°å½•å¤±è´¥: {e}")
            return []

    def get_recent_dialogs(self, limit: int = 5) -> List[Dict]:
        """è·å–æœ€è¿‘çš„å¯¹è¯è®°å½•"""
        try:
            # è·å–æ‰€æœ‰å¯¹è¯è®°å½•
            all_dialogs = self.collections["dialog_logs"].get()
            
            if not all_dialogs or not all_dialogs["metadatas"]:
                return []
            
            # æŒ‰æ—¶é—´æˆ³æ’åº
            dialog_list = []
            for i, metadata in enumerate(all_dialogs["metadatas"]):
                dialog_info = {
                    "id": all_dialogs["ids"][i],
                    "content": all_dialogs["documents"][i],
                    "metadata": metadata,
                    "timestamp": metadata.get("timestamp", ""),
                    "tags": json.loads(metadata.get("tags", "[]"))
                }
                dialog_list.append(dialog_info)
            
            # æŒ‰æ—¶é—´æˆ³å€’åºæ’åº
            dialog_list.sort(key=lambda x: x["timestamp"], reverse=True)
            
            return dialog_list[:limit]
        except Exception as e:
            print(f"âŒ è·å–æœ€è¿‘å¯¹è¯å¤±è´¥: {e}")
            return []

    # === ğŸ§  æ¦‚å¿µçŸ¥è¯†ï¼ˆäº‹å®è®°å¿†ï¼‰===
    # å…ˆå­˜åˆ°ç¼“å†²æ–‡ä»¶ï¼ˆæŒä¹…åŒ–ï¼‰ï¼Œç„¶ååœ¨ç©ºé—²æ—¶ç»è¿‡æ¨¡å‹å¤„ç†åä¿å­˜åˆ°ChromaDB
    def save_fact_memory(self, content: str, tags: List[str], 
                        source: str = "dialog", confidence: float = 1.0,
                        additional_metadata: Optional[Dict] = None):
        """ä¿å­˜äº‹å®è®°å¿†åˆ°ChromaDB"""
        
        # ç”Ÿæˆå”¯ä¸€ID
        fact_id = f"fact_{uuid4().hex}"
        timestamp = get_iso_timestamp()
        current_time = datetime.now()
        natural_time = format_natural_time(current_time)
        
        # æ„å»ºåŒ…å«å®Œæ•´ä¿¡æ¯çš„æ–‡æ¡£å†…å®¹
        tags_str = "ã€".join(tags) if tags else "æ— æ ‡ç­¾"
        
        fact_content = f"""æ—¶é—´ï¼š{natural_time}
æ¥æºï¼š{source}
æ ‡ç­¾ï¼š{tags_str}
å†…å®¹ï¼š{content}"""
        
        # å‡†å¤‡å…ƒæ•°æ®
        metadata = {
            "type": "fact",
            "timestamp": timestamp,
            "source": source,
            "confidence": confidence,
            "tags": json.dumps(tags, ensure_ascii=False),
            "content_length": len(content)
        }
        
        # æ·»åŠ é¢å¤–å…ƒæ•°æ®
        if additional_metadata:
            metadata.update(additional_metadata)
        
        # ä¿å­˜åˆ°ChromaDB
        try:
            self.collections["facts"].add(
                ids=[fact_id],
                metadatas=[metadata],
                documents=[fact_content]
            )
            print(f"âœ… äº‹å®è®°å¿†å·²ä¿å­˜: {content[:30]}... (ç½®ä¿¡åº¦: {confidence})")
            
            # æ›´æ–°æ´»è·ƒæ ‡ç­¾
            self.update_active_tags(tags)
            
            return fact_id
        except Exception as e:
            print(f"âŒ ä¿å­˜äº‹å®è®°å¿†å¤±è´¥: {e}")
            return None

    def search_fact_memory(self, query: str, n_results: int = 3,
                          where_filter: Optional[Dict] = None,
                          similarity_threshold: float = 0.7) -> List[Dict]:
        """æœç´¢äº‹å®è®°å¿†"""
        try:
            search_params = {
                "query_texts": [query],
                "n_results": n_results
            }
            
            if where_filter:
                search_params["where"] = where_filter
            
            results = self.collections["facts"].query(**search_params)
            
            fact_memories = []
            if (results and results["documents"] and results["documents"][0] and
                results["metadatas"] and results["metadatas"][0] and
                results["distances"] and results["distances"][0]):
                
                docs_list = results["documents"][0]
                metadatas_list = results["metadatas"][0]
                distances_list = results["distances"][0]
                ids_list = results["ids"][0]
                
                for i, doc_content in enumerate(docs_list):
                    if i < len(distances_list):
                        distance = distances_list[i]
                        if distance <= (1 - similarity_threshold):
                            metadata = metadatas_list[i]
                            tags = json.loads(metadata.get("tags", "[]"))
                            
                            fact_memory = {
                                "id": ids_list[i],
                                "content": doc_content,
                                "metadata": metadata,
                                "tags": tags,
                                "similarity": 1 - distance,
                                "source": metadata.get("source", ""),
                                "confidence": metadata.get("confidence", 1.0),
                                "timestamp": metadata.get("timestamp", "")
                            }
                            fact_memories.append(fact_memory)
            
            return fact_memories
        except Exception as e:
            print(f"âŒ æœç´¢äº‹å®è®°å¿†å¤±è´¥: {e}")
            return []

    def update_fact_confidence(self, fact_id: str, new_confidence: float):
        """æ›´æ–°äº‹å®è®°å¿†çš„ç½®ä¿¡åº¦"""
        try:
            # è·å–ç°æœ‰è®°å¿†
            result = self.collections["facts"].get(ids=[fact_id])
            if result and result["metadatas"]:
                metadata = result["metadatas"][0]
                metadata["confidence"] = new_confidence
                metadata["last_updated"] = get_timestamp()
                
                # æ›´æ–°è®°å¿†
                self.collections["facts"].update(
                    ids=[fact_id],
                    metadatas=[metadata]
                )
                print(f"âœ… äº‹å®è®°å¿†ç½®ä¿¡åº¦å·²æ›´æ–°: {new_confidence}")
                return True
        except Exception as e:
            print(f"âŒ æ›´æ–°äº‹å®è®°å¿†ç½®ä¿¡åº¦å¤±è´¥: {e}")
        return False

    # === ğŸ“ ç¼“å­˜ç®¡ç†æ–¹æ³• ===
    
    def cache_user_preference(self, content: str, preference_type: str, 
                             tags: List[str], confidence: float = 1.0,
                             additional_metadata: Optional[Dict] = None):
        """ç¼“å­˜ç”¨æˆ·åå¥½ä¿¡æ¯åˆ°æœ¬åœ°JSONæ–‡ä»¶"""
        
        # åˆ›å»ºç¼“å­˜æ¡ç›®
        cache_entry = {
            "id": f"preference_cache_{uuid4().hex}",
            "content": content,
            "preference_type": preference_type,
            "tags": tags,
            "confidence": confidence,
            "timestamp": get_iso_timestamp(),
            "natural_time": format_natural_time(datetime.now()),
            "content_length": len(content),
            "additional_metadata": additional_metadata or {}
        }
        
        # è¯»å–ç°æœ‰ç¼“å­˜
        preferences_cache = self._load_cache_file(PREFERENCE_CACHE_PATH)
        
        # æ·»åŠ æ–°æ¡ç›®
        preferences_cache.append(cache_entry)
        
        # ä¿å­˜ç¼“å­˜
        self._save_cache_file(PREFERENCE_CACHE_PATH, preferences_cache)
        
        print(f"âœ… ç”¨æˆ·åå¥½å·²ç¼“å­˜: {preference_type} - {content[:30]}...")
        return cache_entry["id"]
    
    def cache_fact_memory(self, content: str, tags: List[str], 
                         source: str = "dialog", confidence: float = 1.0,
                         additional_metadata: Optional[Dict] = None):
        """ç¼“å­˜äº‹å®è®°å¿†åˆ°æœ¬åœ°JSONæ–‡ä»¶"""
        
        # åˆ›å»ºç¼“å­˜æ¡ç›®
        cache_entry = {
            "id": f"fact_cache_{uuid4().hex}",
            "content": content,
            "tags": tags,
            "source": source,
            "confidence": confidence,
            "timestamp": get_iso_timestamp(),
            "natural_time": format_natural_time(datetime.now()),
            "content_length": len(content),
            "additional_metadata": additional_metadata or {}
        }
        
        # è¯»å–ç°æœ‰ç¼“å­˜
        facts_cache = self._load_cache_file(FACT_CACHE_PATH)
        
        # æ·»åŠ æ–°æ¡ç›®
        facts_cache.append(cache_entry)
        
        # ä¿å­˜ç¼“å­˜
        self._save_cache_file(FACT_CACHE_PATH, facts_cache)
        
        print(f"âœ… äº‹å®è®°å¿†å·²ç¼“å­˜: {content[:30]}... (ç½®ä¿¡åº¦: {confidence})")
        return cache_entry["id"]
    
    def cache_profile_update(self, profile_data: Dict, source: str = "dialog"):
        """ç¼“å­˜ç”¨æˆ·ç”»åƒæ›´æ–°ä¿¡æ¯åˆ°æœ¬åœ°JSONæ–‡ä»¶"""
        
        # åˆ›å»ºç¼“å­˜æ¡ç›®
        cache_entry = {
            "id": f"profile_cache_{uuid4().hex}",
            "profile_data": profile_data,
            "source": source,
            "timestamp": get_iso_timestamp(),
            "natural_time": format_natural_time(datetime.now())
        }
        
        # è¯»å–ç°æœ‰ç¼“å­˜
        profile_cache = self._load_cache_file(PROFILE_CACHE_PATH)
        
        # æ·»åŠ æ–°æ¡ç›®
        profile_cache.append(cache_entry)
        
        # ä¿å­˜ç¼“å­˜
        self._save_cache_file(PROFILE_CACHE_PATH, profile_cache)
        
        print(f"âœ… ç”¨æˆ·ç”»åƒä¿¡æ¯å·²ç¼“å­˜: {list(profile_data.keys())}")
        return cache_entry["id"]
    
    def _load_cache_file(self, file_path: str) -> List[Dict]:
        """åŠ è½½ç¼“å­˜æ–‡ä»¶"""
        if not os.path.exists(file_path):
            return []
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶æŸåæˆ–ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶: {file_path}")
            return []
    
    def _save_cache_file(self, file_path: str, cache_data: List[Dict]):
        """ä¿å­˜ç¼“å­˜æ–‡ä»¶"""
        try:
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ä¿å­˜ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
    
    def load_preference_cache(self) -> List[Dict]:
        """è¯»å–ç”¨æˆ·åå¥½ç¼“å­˜"""
        return self._load_cache_file(PREFERENCE_CACHE_PATH)
    
    def load_fact_cache(self) -> List[Dict]:
        """è¯»å–äº‹å®è®°å¿†ç¼“å­˜"""
        return self._load_cache_file(FACT_CACHE_PATH)
    
    def load_profile_cache(self) -> List[Dict]:
        """è¯»å–ç”¨æˆ·ç”»åƒç¼“å­˜"""
        return self._load_cache_file(PROFILE_CACHE_PATH)
    
    def clear_preference_cache(self):
        """æ¸…ç©ºç”¨æˆ·åå¥½ç¼“å­˜"""
        self._save_cache_file(PREFERENCE_CACHE_PATH, [])
        print("ğŸ—‘ï¸ ç”¨æˆ·åå¥½ç¼“å­˜å·²æ¸…ç©º")
    
    def clear_fact_cache(self):
        """æ¸…ç©ºäº‹å®è®°å¿†ç¼“å­˜"""
        self._save_cache_file(FACT_CACHE_PATH, [])
        print("ï¿½ï¸ äº‹å®è®°å¿†ç¼“å­˜å·²æ¸…ç©º")
    
    def clear_profile_cache(self):
        """æ¸…ç©ºç”¨æˆ·ç”»åƒç¼“å­˜"""
        self._save_cache_file(PROFILE_CACHE_PATH, [])
        print("ğŸ—‘ï¸ ç”¨æˆ·ç”»åƒç¼“å­˜å·²æ¸…ç©º")
    
    def get_cache_status(self) -> Dict[str, int]:
        """è·å–å„ç¼“å­˜æ–‡ä»¶çš„çŠ¶æ€"""
        status = {
            "preferences_cache": len(self._load_cache_file(PREFERENCE_CACHE_PATH)),
            "facts_cache": len(self._load_cache_file(FACT_CACHE_PATH)),
            "profile_cache": len(self._load_cache_file(PROFILE_CACHE_PATH))
        }
        
        total = sum(status.values())
        print(f"ğŸ“Š ç¼“å­˜çŠ¶æ€: åå¥½ {status['preferences_cache']} æ¡ï¼Œäº‹å® {status['facts_cache']} æ¡ï¼Œç”»åƒ {status['profile_cache']} æ¡ï¼Œæ€»è®¡ {total} æ¡")
        
        return status
    
    def clear_all_caches(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        self.clear_preference_cache()
        self.clear_fact_cache()
        self.clear_profile_cache()
        print("ğŸ—‘ï¸ æ‰€æœ‰ç¼“å­˜å·²æ¸…ç©º")

    # === ğŸ¯ ç”¨æˆ·åå¥½ä¿¡æ¯ ===
    # å…ˆå­˜åˆ°ç¼“å†²æ–‡ä»¶ï¼ˆæŒä¹…åŒ–ï¼‰ï¼Œç„¶ååœ¨ç©ºé—²æ—¶ç»è¿‡æ¨¡å‹å¤„ç†åä¿å­˜åˆ°ChromaDB
    def save_user_preference(self, content: str, preference_type: str, 
                            tags: List[str], additional_metadata: Optional[Dict] = None):
        """ä¿å­˜ç”¨æˆ·åå¥½ä¿¡æ¯åˆ°ChromaDB"""
        
        # ç”Ÿæˆå”¯ä¸€ID
        preference_id = f"preference_{uuid4().hex}"
        timestamp = get_iso_timestamp()
        current_time = datetime.now()
        natural_time = format_natural_time(current_time)
        
        # æ„å»ºåŒ…å«å®Œæ•´ä¿¡æ¯çš„æ–‡æ¡£å†…å®¹
        tags_str = "ã€".join(tags) if tags else "æ— æ ‡ç­¾"
        
        preference_content = f"""æ—¶é—´ï¼š{natural_time}
ç±»å‹ï¼š{preference_type}
æ ‡ç­¾ï¼š{tags_str}
å†…å®¹ï¼š{content}"""
        
        # å‡†å¤‡å…ƒæ•°æ®
        metadata = {
            "type": preference_type,
            "tags": json.dumps(tags, ensure_ascii=False),
            "timestamp": timestamp,
            "content_length": len(content)
        }
        
        # æ·»åŠ é¢å¤–å…ƒæ•°æ®
        if additional_metadata:
            metadata.update(additional_metadata)
        
        # ä¿å­˜åˆ°ChromaDB
        try:
            self.collections["user_preferences"].add(
                ids=[preference_id],
                metadatas=[metadata],
                documents=[preference_content]
            )
            print(f"âœ… ç”¨æˆ·åå¥½å·²ä¿å­˜: {preference_type} - {content[:30]}...")
            
            # æ›´æ–°æ´»è·ƒæ ‡ç­¾
            self.update_active_tags(tags)
            
            return preference_id
        except Exception as e:
            print(f"âŒ ä¿å­˜ç”¨æˆ·åå¥½å¤±è´¥: {e}")
            return None

    def search_user_preferences(self, query: str, n_results: int = 5,
                               where_filter: Optional[Dict] = None,
                               similarity_threshold: float = 0.7) -> List[Dict]:
        """æœç´¢ç”¨æˆ·åå¥½ä¿¡æ¯"""
        try:
            search_params = {
                "query_texts": [query],
                "n_results": n_results
            }
            
            if where_filter:
                search_params["where"] = where_filter
            
            results = self.collections["user_preferences"].query(**search_params)
            
            preference_memories = []
            if (results and results["documents"] and results["documents"][0] and
                results["metadatas"] and results["metadatas"][0] and
                results["distances"] and results["distances"][0]):
                
                docs_list = results["documents"][0]
                metadatas_list = results["metadatas"][0]
                distances_list = results["distances"][0]
                ids_list = results["ids"][0]
                
                for i, doc_content in enumerate(docs_list):
                    if i < len(distances_list):
                        distance = distances_list[i]
                        if distance <= (1 - similarity_threshold):
                            metadata = metadatas_list[i]
                            tags = json.loads(metadata.get("tags", "[]"))
                            
                            preference_memory = {
                                "id": ids_list[i],
                                "content": doc_content,
                                "metadata": metadata,
                                "tags": tags,
                                "similarity": 1 - distance,
                                "type": metadata.get("type", ""),
                                "timestamp": metadata.get("timestamp", "")
                            }
                            preference_memories.append(preference_memory)
            
            return preference_memories
        except Exception as e:
            print(f"âŒ æœç´¢ç”¨æˆ·åå¥½å¤±è´¥: {e}")
            return []

    def get_preferences_by_type(self, preference_type: str, limit: int = 10) -> List[Dict]:
        """æ ¹æ®ç±»å‹è·å–ç”¨æˆ·åå¥½"""
        return self.search_user_preferences(
            query="", 
            n_results=limit,
            where_filter={"type": preference_type}
        )

    # === ğŸ¯ é‡å¤§äº‹ä»¶ç®¡ç† ===
    def save_important_event(self, content: str, event_type: str, summary: str,
                            tags: List[str], additional_metadata: Optional[Dict] = None):
        """ä¿å­˜é‡å¤§äº‹ä»¶åˆ°ChromaDB"""
        
        # ç”Ÿæˆå”¯ä¸€ID
        event_id = f"event_{uuid4().hex}"
        timestamp = get_iso_timestamp()
        current_time = datetime.now()
        natural_time = format_natural_time(current_time)
        
        # æ„å»ºåŒ…å«å®Œæ•´ä¿¡æ¯çš„æ–‡æ¡£å†…å®¹
        tags_str = "ã€".join(tags) if tags else "æ— æ ‡ç­¾"
        
        event_content = f"""æ—¶é—´ï¼š{natural_time}
äº‹ä»¶ç±»å‹ï¼š{event_type}
æ¦‚è¿°ï¼š{summary}
æ ‡ç­¾ï¼š{tags_str}
è¯¦ç»†å†…å®¹ï¼š{content}"""
        
        # å‡†å¤‡å…ƒæ•°æ®
        metadata = {
            "event_type": event_type,
            "summary": summary,
            "tags": json.dumps(tags, ensure_ascii=False),
            "timestamp": timestamp,
            "content_length": len(content)
        }
        
        # æ·»åŠ é¢å¤–å…ƒæ•°æ®
        if additional_metadata:
            metadata.update(additional_metadata)
        
        # ä¿å­˜åˆ°ChromaDB
        try:
            self.collections["important_events"].add(
                ids=[event_id],
                metadatas=[metadata],
                documents=[event_content]
            )
            print(f"âœ… é‡å¤§äº‹ä»¶å·²ä¿å­˜: {event_type} - {summary}")
            
            # æ›´æ–°æ´»è·ƒæ ‡ç­¾
            self.update_active_tags(tags)
            
            return event_id
        except Exception as e:
            print(f"âŒ ä¿å­˜é‡å¤§äº‹ä»¶å¤±è´¥: {e}")
            return None

    def search_important_events(self, query: str, n_results: int = 5,
                               where_filter: Optional[Dict] = None,
                               similarity_threshold: float = 0.7) -> List[Dict]:
        """æœç´¢é‡å¤§äº‹ä»¶"""
        try:
            search_params = {
                "query_texts": [query],
                "n_results": n_results
            }
            
            if where_filter:
                search_params["where"] = where_filter
            
            results = self.collections["important_events"].query(**search_params)
            
            event_memories = []
            if (results and results["documents"] and results["documents"][0] and
                results["metadatas"] and results["metadatas"][0] and
                results["distances"] and results["distances"][0]):
                
                docs_list = results["documents"][0]
                metadatas_list = results["metadatas"][0]
                distances_list = results["distances"][0]
                ids_list = results["ids"][0]
                
                for i, doc_content in enumerate(docs_list):
                    if i < len(distances_list):
                        distance = distances_list[i]
                        if distance <= (1 - similarity_threshold):
                            metadata = metadatas_list[i]
                            tags = json.loads(metadata.get("tags", "[]"))
                            
                            event_memory = {
                                "id": ids_list[i],
                                "content": doc_content,
                                "metadata": metadata,
                                "tags": tags,
                                "similarity": 1 - distance,
                                "event_type": metadata.get("event_type", ""),
                                "summary": metadata.get("summary", ""),
                                "timestamp": metadata.get("timestamp", "")
                            }
                            event_memories.append(event_memory)
            
            return event_memories
        except Exception as e:
            print(f"âŒ æœç´¢é‡å¤§äº‹ä»¶å¤±è´¥: {e}")
            return []

    def get_events_by_type(self, event_type: str, limit: int = 10) -> List[Dict]:
        """æ ¹æ®ç±»å‹è·å–é‡å¤§äº‹ä»¶"""
        return self.search_important_events(
            query="", 
            n_results=limit,
            where_filter={"event_type": event_type}
        )

    # === â° è¿‘æœŸå…³æ³¨äº‹ä»¶ç®¡ç† ===
    def save_temp_focus_event(self, content: str, event_time: str, 
                             expire_time: str, tags: List[str]):
        """ä¿å­˜è¿‘æœŸå…³æ³¨äº‹ä»¶"""
        temp_event = {
            "created_at": get_iso_timestamp(),
            "event_time": event_time,
            "expire_time": expire_time,
            "content": content,
            "tags": tags
        }
        
        # è¯»å–ç°æœ‰äº‹ä»¶
        temp_events = self.load_temp_focus_events()
        
        # æ·»åŠ æ–°äº‹ä»¶
        temp_events.append(temp_event)
        
        # ä¿å­˜æ›´æ–°åçš„äº‹ä»¶åˆ—è¡¨
        try:
            with open(TEMP_FOCUS_EVENTS_PATH, "w", encoding="utf-8") as f:
                json.dump(temp_events, f, ensure_ascii=False, indent=2)
            print(f"âœ… è¿‘æœŸå…³æ³¨äº‹ä»¶å·²ä¿å­˜: {content[:30]}...")
            
            # æ›´æ–°æ´»è·ƒæ ‡ç­¾
            self.update_active_tags(tags)
            
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜è¿‘æœŸå…³æ³¨äº‹ä»¶å¤±è´¥: {e}")
            return False

    def load_temp_focus_events(self) -> List[Dict]:
        """åŠ è½½è¿‘æœŸå…³æ³¨äº‹ä»¶ï¼ˆè‡ªåŠ¨æ¸…ç†è¿‡æœŸäº‹ä»¶ï¼‰"""
        if not os.path.exists(TEMP_FOCUS_EVENTS_PATH):
            return []
        
        try:
            with open(TEMP_FOCUS_EVENTS_PATH, encoding="utf-8") as f:
                events = json.load(f)
            
            # è¿‡æ»¤æ‰è¿‡æœŸäº‹ä»¶
            current_time = datetime.now()
            valid_events = []
            
            for event in events:
                try:
                    expire_time = datetime.fromisoformat(event["expire_time"])
                    if current_time < expire_time:
                        valid_events.append(event)
                except:
                    # å¦‚æœæ—¶é—´è§£æå¤±è´¥ï¼Œä¿ç•™äº‹ä»¶
                    valid_events.append(event)
            
            # å¦‚æœæœ‰äº‹ä»¶è¢«æ¸…ç†ï¼Œæ›´æ–°æ–‡ä»¶
            if len(valid_events) != len(events):
                with open(TEMP_FOCUS_EVENTS_PATH, "w", encoding="utf-8") as f:
                    json.dump(valid_events, f, ensure_ascii=False, indent=2)
                print(f"ğŸ§¹ å·²æ¸…ç† {len(events) - len(valid_events)} ä¸ªè¿‡æœŸçš„å…³æ³¨äº‹ä»¶")
            
            return valid_events
        except Exception as e:
            print(f"âŒ åŠ è½½è¿‘æœŸå…³æ³¨äº‹ä»¶å¤±è´¥: {e}")
            return []

    def update_temp_focus_event_expire_time(self, event_index: int, new_expire_time: str):
        """æ›´æ–°è¿‘æœŸå…³æ³¨äº‹ä»¶çš„è¿‡æœŸæ—¶é—´"""
        events = self.load_temp_focus_events()
        
        if 0 <= event_index < len(events):
            events[event_index]["expire_time"] = new_expire_time
            
            try:
                with open(TEMP_FOCUS_EVENTS_PATH, "w", encoding="utf-8") as f:
                    json.dump(events, f, ensure_ascii=False, indent=2)
                print(f"âœ… äº‹ä»¶è¿‡æœŸæ—¶é—´å·²æ›´æ–°: {new_expire_time}")
                return True
            except Exception as e:
                print(f"âŒ æ›´æ–°äº‹ä»¶è¿‡æœŸæ—¶é—´å¤±è´¥: {e}")
        else:
            print(f"âŒ äº‹ä»¶ç´¢å¼• {event_index} è¶…å‡ºèŒƒå›´")
        
        return False

    def get_active_focus_events(self) -> List[Dict]:
        """è·å–å½“å‰æœ‰æ•ˆçš„å…³æ³¨äº‹ä»¶"""
        return self.load_temp_focus_events()

    def clear_expired_focus_events(self):
        """æ‰‹åŠ¨æ¸…ç†è¿‡æœŸçš„å…³æ³¨äº‹ä»¶"""
        valid_events = self.load_temp_focus_events()  # è¿™ä¼šè‡ªåŠ¨æ¸…ç†è¿‡æœŸäº‹ä»¶
        return len(valid_events)

    # === ğŸ”– æ´»è·ƒæ ‡ç­¾ ===
    def update_active_tags(self, new_tags: List[str]):
        """æ›´æ–°æ´»è·ƒæ ‡ç­¾ç»Ÿè®¡"""
        tags = {}
        if os.path.exists(ACTIVE_TAGS_PATH):
            try:
                with open(ACTIVE_TAGS_PATH, encoding="utf-8") as f:
                    data = json.load(f)
                    tags = data.get("tags", {})
            except:
                tags = {}
        
        # æ›´æ–°æ ‡ç­¾è®¡æ•°
        for tag in new_tags:
            tags[tag] = tags.get(tag, 0) + 1
        
        # ä¿å­˜æ ‡ç­¾æ•°æ®
        data = {
            "type": "active_tags", 
            "tags": tags, 
            "last_update": get_timestamp(),
            "total_tags": len(tags),
            "most_frequent": max(tags.items(), key=lambda x: x[1]) if tags else None
        }
        
        with open(ACTIVE_TAGS_PATH, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def get_active_tags(self, top_n: int = 10) -> Dict:
        """è·å–æœ€æ´»è·ƒçš„æ ‡ç­¾"""
        if not os.path.exists(ACTIVE_TAGS_PATH):
            return {"tags": {}, "top_tags": []}
        
        with open(ACTIVE_TAGS_PATH, encoding="utf-8") as f:
            data = json.load(f)
            tags = data.get("tags", {})
            
            # æŒ‰é¢‘ç‡æ’åº
            sorted_tags = sorted(tags.items(), key=lambda x: x[1], reverse=True)
            top_tags = sorted_tags[:top_n]
            
            return {
                "tags": tags,
                "top_tags": top_tags,
                "total_count": sum(tags.values()),
                "unique_count": len(tags),
                "last_update": data.get("last_update", "")
            }

    # === ğŸ” ç»¼åˆæœç´¢åŠŸèƒ½ ===
    def comprehensive_search(self, query: str, search_dialogs: bool = True, 
                           search_facts: bool = True, search_preferences: bool = True,
                           search_events: bool = True, n_results: int = 5) -> Dict:
        """ç»¼åˆæœç´¢æ‰€æœ‰ç±»å‹çš„è®°å¿†"""
        results = {
            "query": query,
            "timestamp": get_timestamp(),
            "dialog_memories": [],
            "fact_memories": [],
            "preference_memories": [],
            "event_memories": [],
            "focus_events": []
        }
        
        if search_dialogs:
            results["dialog_memories"] = self.search_dialog_logs(query, n_results)
        
        if search_facts:
            results["fact_memories"] = self.search_fact_memory(query, n_results)
            
        if search_preferences:
            results["preference_memories"] = self.search_user_preferences(query, n_results)
            
        if search_events:
            results["event_memories"] = self.search_important_events(query, n_results)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³çš„å…³æ³¨äº‹ä»¶
        focus_events = self.get_active_focus_events()
        relevant_focus_events = []
        for event in focus_events:
            if (query.lower() in event["content"].lower() or 
                any(tag.lower() in query.lower() for tag in event["tags"])):
                relevant_focus_events.append(event)
        results["focus_events"] = relevant_focus_events
        
        return results

    # === ğŸ“Š ç»Ÿè®¡å’Œç®¡ç†åŠŸèƒ½ ===
    def get_memory_statistics(self) -> Dict:
        """è·å–è®°å¿†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "timestamp": get_timestamp(),
            "dialog_count": 0,
            "fact_count": 0,
            "preference_count": 0,
            "event_count": 0,
            "focus_event_count": 0,
            "user_profile_exists": os.path.exists(PROFILE_PATH),
            "active_tags": self.get_active_tags(5)
        }
        
        try:
            # ç»Ÿè®¡å¯¹è¯è®°å½•æ•°é‡
            dialogs = self.collections["dialog_logs"].count()
            stats["dialog_count"] = dialogs
        except:
            pass
        
        try:
            # ç»Ÿè®¡äº‹å®è®°å¿†æ•°é‡
            facts = self.collections["facts"].count()
            stats["fact_count"] = facts
        except:
            pass
            
        try:
            # ç»Ÿè®¡ç”¨æˆ·åå¥½æ•°é‡
            preferences = self.collections["user_preferences"].count()
            stats["preference_count"] = preferences
        except:
            pass
            
        try:
            # ç»Ÿè®¡é‡å¤§äº‹ä»¶æ•°é‡
            events = self.collections["important_events"].count()
            stats["event_count"] = events
        except:
            pass
            
        # ç»Ÿè®¡å…³æ³¨äº‹ä»¶æ•°é‡
        focus_events = self.get_active_focus_events()
        stats["focus_event_count"] = len(focus_events)
        
        return stats

    def cleanup_old_memories(self, days_threshold: int = 30, 
                           importance_threshold: float = 0.3):
        """æ¸…ç†æ—§çš„ä½é‡è¦æ€§è®°å¿†ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰"""
        print(f"ğŸ§¹ å¼€å§‹æ¸…ç† {days_threshold} å¤©å‰é‡è¦æ€§ä½äº {importance_threshold} çš„è®°å¿†...")
        
        from datetime import timedelta
        cutoff_date = (datetime.now() - timedelta(days=days_threshold)).isoformat()
        
        # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„æ¸…ç†é€»è¾‘
        # æ³¨æ„ï¼šChromaDBçš„åˆ é™¤æ“ä½œéœ€è¦è°¨æ…å¤„ç†
        print("âš ï¸ æ¸…ç†åŠŸèƒ½å¾…å®ç°ï¼Œå»ºè®®æ‰‹åŠ¨ç®¡ç†é‡è¦è®°å¿†")


# === ğŸ§© å…¨å±€å®ä¾‹å’Œä¾¿æ·å‡½æ•° ===

# åˆ›å»ºå…¨å±€è®°å¿†ç³»ç»Ÿå®ä¾‹
memory_system = MemorySystem()

# ä¾¿æ·å‡½æ•°
def save_dialog_log(user_input: str, ai_response: str, topic: str, 
                   sentiment: str, importance: float, tags: List[str]):
    """ä¾¿æ·çš„å¯¹è¯è®°å½•ä¿å­˜å‡½æ•°"""
    return memory_system.save_dialog_log(user_input, ai_response, topic, 
                                       sentiment, importance, tags)

def save_fact_memory(content: str, tags: List[str], source: str = "dialog"):
    """ä¾¿æ·çš„äº‹å®è®°å¿†ä¿å­˜å‡½æ•°"""
    return memory_system.save_fact_memory(content, tags, source)

def save_user_preference(content: str, preference_type: str, tags: List[str]):
    """ä¾¿æ·çš„ç”¨æˆ·åå¥½ä¿å­˜å‡½æ•°"""
    return memory_system.save_user_preference(content, preference_type, tags)

def save_important_event(content: str, event_type: str, summary: str, tags: List[str]):
    """ä¾¿æ·çš„é‡å¤§äº‹ä»¶ä¿å­˜å‡½æ•°"""
    return memory_system.save_important_event(content, event_type, summary, tags)

def save_temp_focus_event(content: str, event_time: str, expire_time: str, tags: List[str]):
    """ä¾¿æ·çš„ä¸´æ—¶å…³æ³¨äº‹ä»¶ä¿å­˜å‡½æ•°"""
    return memory_system.save_temp_focus_event(content, event_time, expire_time, tags)

def search_memories(query: str, n_results: int = 5):
    """ä¾¿æ·çš„è®°å¿†æœç´¢å‡½æ•°"""
    return memory_system.comprehensive_search(query, n_results=n_results)

def parse_response_with_llm(response_text: str) -> Dict:
    """æ¨¡æ‹ŸLLMè§£æå“åº”ï¼ˆå¾…æ¥å…¥çœŸå®LLMï¼‰"""
    # è¿™é‡Œåº”è¯¥è°ƒç”¨ä½ çš„LLMæ¥è§£æç”¨æˆ·è¾“å…¥
    # ç›®å‰è¿”å›ç¤ºä¾‹æ•°æ®
    return {
        "topic": "æµè¤å’Œæ˜Ÿæ˜Ÿ",
        "sentiment": "æ¸©æŸ”",
        "importance": 0.9,
        "tags": ["æµè¤", "æµªæ¼«", "å´©åæ˜Ÿç©¹é“é“"]
    }

# === ğŸ¯ å…¨å±€ä¾¿æ·å‡½æ•°ï¼ˆç¼“å­˜ç‰ˆæœ¬ï¼‰===

# å®ä¾‹åŒ–å…¨å±€è®°å¿†ç³»ç»Ÿ
_global_memory_system = None

def get_memory_system():
    """è·å–å…¨å±€è®°å¿†ç³»ç»Ÿå®ä¾‹"""
    global _global_memory_system
    if _global_memory_system is None:
        _global_memory_system = MemorySystem()
    return _global_memory_system

# ç¼“å­˜ç›¸å…³ä¾¿æ·å‡½æ•°
def cache_user_preference(content: str, preference_type: str, tags: List[str], 
                         confidence: float = 1.0, additional_metadata: Optional[Dict] = None):
    """ä¾¿æ·å‡½æ•°ï¼šç¼“å­˜ç”¨æˆ·åå¥½ä¿¡æ¯"""
    memory_system = get_memory_system()
    return memory_system.cache_user_preference(content, preference_type, tags, confidence, additional_metadata)

def cache_fact_memory(content: str, tags: List[str], source: str = "dialog", 
                     confidence: float = 1.0, additional_metadata: Optional[Dict] = None):
    """ä¾¿æ·å‡½æ•°ï¼šç¼“å­˜äº‹å®è®°å¿†"""
    memory_system = get_memory_system()
    return memory_system.cache_fact_memory(content, tags, source, confidence, additional_metadata)

def cache_profile_update(profile_data: Dict, source: str = "dialog"):
    """ä¾¿æ·å‡½æ•°ï¼šç¼“å­˜ç”¨æˆ·ç”»åƒæ›´æ–°"""
    memory_system = get_memory_system()
    return memory_system.cache_profile_update(profile_data, source)

def load_preference_cache():
    """ä¾¿æ·å‡½æ•°ï¼šè¯»å–ç”¨æˆ·åå¥½ç¼“å­˜"""
    memory_system = get_memory_system()
    return memory_system.load_preference_cache()

def load_fact_cache():
    """ä¾¿æ·å‡½æ•°ï¼šè¯»å–äº‹å®è®°å¿†ç¼“å­˜"""
    memory_system = get_memory_system()
    return memory_system.load_fact_cache()

def load_profile_cache():
    """ä¾¿æ·å‡½æ•°ï¼šè¯»å–ç”¨æˆ·ç”»åƒç¼“å­˜"""
    memory_system = get_memory_system()
    return memory_system.load_profile_cache()

def clear_preference_cache():
    """ä¾¿æ·å‡½æ•°ï¼šæ¸…ç©ºç”¨æˆ·åå¥½ç¼“å­˜"""
    memory_system = get_memory_system()
    return memory_system.clear_preference_cache()

def clear_fact_cache():
    """ä¾¿æ·å‡½æ•°ï¼šæ¸…ç©ºäº‹å®è®°å¿†ç¼“å­˜"""
    memory_system = get_memory_system()
    return memory_system.clear_fact_cache()

def clear_profile_cache():
    """ä¾¿æ·å‡½æ•°ï¼šæ¸…ç©ºç”¨æˆ·ç”»åƒç¼“å­˜"""
    memory_system = get_memory_system()
    return memory_system.clear_profile_cache()

def get_cache_status():
    """ä¾¿æ·å‡½æ•°ï¼šè·å–ç¼“å­˜çŠ¶æ€"""
    memory_system = get_memory_system()
    return memory_system.get_cache_status()

def clear_all_caches():
    """ä¾¿æ·å‡½æ•°ï¼šæ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
    memory_system = get_memory_system()
    return memory_system.clear_all_caches()

# ç¤ºä¾‹ä½¿ç”¨ç¼“å­˜åŠŸèƒ½
def test_cache_system():
    """æµ‹è¯•ç¼“å­˜ç³»ç»Ÿ"""
    print("ğŸ§ª æµ‹è¯•ç¼“å­˜ç³»ç»Ÿ...")
    
    # æµ‹è¯•ç¼“å­˜ç”¨æˆ·åå¥½
    cache_user_preference(
        content="ç”¨æˆ·å–œæ¬¢å¬å¤å…¸éŸ³ä¹ï¼Œç‰¹åˆ«æ˜¯è´å¤šèŠ¬çš„ä½œå“",
        preference_type="éŸ³ä¹åå¥½",
        tags=["éŸ³ä¹", "å¤å…¸", "è´å¤šèŠ¬"],
        confidence=0.9
    )
    
    cache_user_preference(
        content="ç”¨æˆ·å–œæ¬¢å¬å¤å…¸éŸ³ä¹å’Œè½»éŸ³ä¹",
        preference_type="éŸ³ä¹åå¥½", 
        tags=["éŸ³ä¹", "å¤å…¸", "è½»éŸ³ä¹"],
        confidence=0.8
    )
    
    # æµ‹è¯•ç¼“å­˜äº‹å®è®°å¿†
    cache_fact_memory(
        content="ç”¨æˆ·çš„ç”Ÿæ—¥æ˜¯3æœˆ15æ—¥",
        tags=["ä¸ªäººä¿¡æ¯", "ç”Ÿæ—¥"],
        source="å¯¹è¯",
        confidence=1.0
    )
    
    cache_fact_memory(
        content="ç”¨æˆ·ç”Ÿæ—¥åœ¨æ˜¥å¤©ï¼Œå…·ä½“æ˜¯3æœˆä¸­æ—¬",
        tags=["ä¸ªäººä¿¡æ¯", "ç”Ÿæ—¥", "æ˜¥å¤©"],
        source="å¯¹è¯",
        confidence=0.7
    )
    
    # æµ‹è¯•ç¼“å­˜ç”¨æˆ·ç”»åƒ
    cache_profile_update({
        "age_range": "25-30",
        "interests": ["éŸ³ä¹", "è¯»ä¹¦", "æ—…è¡Œ"],
        "personality": "å†…å‘ä½†å‹å–„"
    })
    
    cache_profile_update({
        "location": "åŒ—äº¬",
        "occupation": "ç¨‹åºå‘˜"
    })
    
    # æ˜¾ç¤ºç¼“å­˜çŠ¶æ€
    get_cache_status()
    
    # æµ‹è¯•è¯»å–ç¼“å­˜
    print("\nï¿½ æµ‹è¯•è¯»å–ç¼“å­˜:")
    prefs = load_preference_cache()
    facts = load_fact_cache()
    profile = load_profile_cache()
    
    print(f"ç”¨æˆ·åå¥½ç¼“å­˜: {len(prefs)} æ¡")
    print(f"äº‹å®è®°å¿†ç¼“å­˜: {len(facts)} æ¡")
    print(f"ç”¨æˆ·ç”»åƒç¼“å­˜: {len(profile)} æ¡")
    
    # å¯ä»¥é€‰æ‹©æ€§æ¸…ç†æŸä¸ªç¼“å­˜
    # clear_preference_cache()  # æ³¨é‡Šæ‰ï¼Œä»…åšæ¼”ç¤º
    
    print("âœ… ç¼“å­˜ç³»ç»Ÿæµ‹è¯•å®Œæˆ!")

# === ğŸ§ª ç”¨ä¾‹æµ‹è¯• ===
if __name__ == "__main__":
    print("ğŸš€ åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿæµ‹è¯•...")
    
    # æµ‹è¯•ç”¨æˆ·ç”»åƒï¼ˆæŒ‰ç…§ä¿®æ”¹æ–¹æ¡ˆçš„ç»“æ„ï¼‰
    user_profile_example = {
        "basic": {
            "name": "å°æ˜",
            "gender": "ç”·",
            "birthday": "2000-01-01"
        },
        "identity": {
            "roles": ["è®¡ç®—æœºä¸“ä¸šå­¦ç”Ÿ", "å‰ä»–åˆå­¦è€…"],
            "job": "å­¦ç”Ÿ",
            "dream": "åˆ¶ä½œè‡ªå·±çš„AIä¼´ä¾£",
            "care_about_people": ["å¦ˆå¦ˆ", "é—ºèœœ"]
        }
    }
    memory_system.save_user_profile(user_profile_example)
    
    # æµ‹è¯•å¯¹è¯è®°å½•ä¿å­˜
    memory_data = parse_response_with_llm("æˆ‘çœŸçš„è¶…çº§å–œæ¬¢æµè¤ï¼Œå¥¹å°±åƒä¸€é¢—åœ¨å¤œé‡Œå‘å…‰çš„æ˜Ÿæ˜Ÿâ€¦â€¦")
    
    dialog_id = memory_system.save_dialog_log(
        user_input="æˆ‘çœŸçš„è¶…çº§å–œæ¬¢æµè¤ï¼Œå¥¹å°±åƒä¸€é¢—åœ¨å¤œé‡Œå‘å…‰çš„æ˜Ÿæ˜Ÿâ€¦â€¦",
        ai_response="å“‡ï¼Œä¸»äººè¯´å¾—å¥½æ¸©æŸ”å–µ~ æµè¤çœŸçš„å°±æ˜¯é‚£æ ·æ²»æ„ˆäººå¿ƒçš„å­˜åœ¨~",
        topic=memory_data["topic"],
        sentiment=memory_data["sentiment"],
        importance=memory_data["importance"],
        tags=memory_data["tags"]
    )
    
    # æµ‹è¯•äº‹å®è®°å¿†ä¿å­˜
    fact_id = memory_system.save_fact_memory(
        "ä¸»äººå–œæ¬¢æµè¤å’Œæ˜Ÿæ˜Ÿçš„æ„è±¡ï¼Œè®¤ä¸ºå¥¹å¾ˆæ²»æ„ˆã€‚", 
        memory_data["tags"]
    )
    
    # æµ‹è¯•ç”¨æˆ·åå¥½ä¿å­˜
    preference_id = memory_system.save_user_preference(
        "ç”¨æˆ·å–œæ¬¢è½»æ¾æç¬‘çš„åŠ¨ç”»ç•ªå‰§ï¼Œæœ€å–œæ¬¢çš„æ˜¯ã€Šå­¤ç‹¬æ‘‡æ»šã€‹ã€‚",
        "å¨±ä¹",
        ["ç•ªå‰§", "æç¬‘", "è½»æ¾"]
    )
    
    # æµ‹è¯•é‡å¤§äº‹ä»¶ä¿å­˜
    event_id = memory_system.save_important_event(
        "ç”¨æˆ·å¸Œæœ›è€ƒä¸Šç†æƒ³çš„ç ”ç©¶ç”Ÿé™¢æ ¡ï¼Œç›®å‰æ­£åœ¨å‡†å¤‡å¤ä¹ ï¼Œæ¯å¤©éƒ½æœ‰å­¦ä¹ è®¡åˆ’ã€‚",
        "è€ƒè¯•",
        "ç”¨æˆ·è®¡åˆ’è€ƒç ”",
        ["ç›®æ ‡", "é•¿æœŸ", "ç´§å¼ "]
    )
    
    # æµ‹è¯•ä¸´æ—¶å…³æ³¨äº‹ä»¶ä¿å­˜
    temp_event_saved = memory_system.save_temp_focus_event(
        "ç”¨æˆ·å°†äº7æœˆ8æ—¥å‚åŠ è‹±è¯­å››çº§è€ƒè¯•",
        "2025-07-08",
        "2025-07-09T23:59:59",
        ["è€ƒè¯•", "ç´§å¼ ", "çŸ­æœŸç„¦ç‚¹"]
    )
    
    # æµ‹è¯•æœç´¢åŠŸèƒ½
    print("\nğŸ” æµ‹è¯•ç»¼åˆæœç´¢åŠŸèƒ½:")
    search_results = memory_system.comprehensive_search("æµè¤")
    print(f"æœç´¢åˆ° {len(search_results['dialog_memories'])} æ¡å¯¹è¯è®°å½•")
    print(f"æœç´¢åˆ° {len(search_results['fact_memories'])} æ¡äº‹å®è®°å¿†")
    print(f"æœç´¢åˆ° {len(search_results['preference_memories'])} æ¡ç”¨æˆ·åå¥½")
    print(f"æœç´¢åˆ° {len(search_results['event_memories'])} æ¡é‡å¤§äº‹ä»¶")
    print(f"æœç´¢åˆ° {len(search_results['focus_events'])} æ¡å…³æ³¨äº‹ä»¶")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š è®°å¿†ç³»ç»Ÿç»Ÿè®¡:")
    stats = memory_system.get_memory_statistics()
    print(f"å¯¹è¯è®°å½•: {stats['dialog_count']} æ¡")
    print(f"äº‹å®è®°å¿†: {stats['fact_count']} æ¡")
    print(f"ç”¨æˆ·åå¥½: {stats['preference_count']} æ¡")
    print(f"é‡å¤§äº‹ä»¶: {stats['event_count']} æ¡")
    print(f"å…³æ³¨äº‹ä»¶: {stats['focus_event_count']} æ¡")
    print(f"æ´»è·ƒæ ‡ç­¾: {stats['active_tags']['unique_count']} ä¸ª")
    
    # æµ‹è¯•å…³æ³¨äº‹ä»¶ç®¡ç†
    print("\nâ° æµ‹è¯•å…³æ³¨äº‹ä»¶ç®¡ç†:")
    focus_events = memory_system.get_active_focus_events()
    for i, event in enumerate(focus_events):
        print(f"äº‹ä»¶ {i}: {event['content'][:50]}... (è¿‡æœŸæ—¶é—´: {event['expire_time']})")
    
    # æµ‹è¯•ç¼“å­˜ç³»ç»Ÿ
    print("\nğŸ—‚ï¸ æµ‹è¯•ç¼“å­˜ç³»ç»Ÿ:")
    test_cache_system()
    
    print("\nâœ… è®°å¿†ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")